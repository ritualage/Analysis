{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritualage/Analysis/blob/main/examples/auto_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caZYLROd8xnV"
      },
      "source": [
        "To train a model for your custom task, click _Runtime_ and press _Run all_. Make sure you've enabled a free Tesla T4 GPU!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "**Custom Task Training with ART**\n",
        "\n",
        "This notebook shows how to train a Qwen 2.5 7B model to perform any single-turn task you describe - no labeled data needed! Simply describe what you want the model to learn, and this notebook will:\n",
        "\n",
        "1. Generate diverse input examples for your task\n",
        "2. Create an appropriate system prompt\n",
        "3. Train the model using RULER's automatic evaluation\n",
        "4. Test the trained model on new inputs\n",
        "\n",
        "RULER learns what makes a good output purely from your task description - no expected outputs required!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OsrwCDQ5cviC"
      },
      "outputs": [],
      "source": [
        "# @title üíø Installation\n",
        "\n",
        "!uv pip install -q openpipe-art==0.3.11.post2 langchain-core tenacity \"gql<4\" --prerelease allow --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8b8kgQ69ZDM"
      },
      "source": [
        "<a name=\"Configuration\"></a>\n",
        "\n",
        "### üéØ Configuration - Edit These Settings\n",
        "\n",
        "Add an OpenRouter key and customize your training by modifying the values below.\n",
        "\n",
        "By default your model will be trained to fix grammar and spelling errors, similar to the Grammarly service. To teach your model another skill, set `TASK_DESCRIPTION` to one of the descriptions under **Advanced Settings**, or write your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so6r1_OG9en3"
      },
      "outputs": [],
      "source": [
        "# Required - Used for generating training inputs and RULER evaluation\n",
        "OPENROUTER_API_KEY = \"opk_ad81963808d785a68f3089c387529cb713788dfe3c\"\n",
        "\n",
        "# Optional - Enables metric logging\n",
        "WANDB_API_KEY = \"042728052ab3f603351cb30421db5287b62c8733\"\n",
        "\n",
        "# Describe your custom task (be specific!)\n",
        "GRAMMARLY_TASK_DESCRIPTION = \"\"\"\n",
        "Read the user's text and check if it has any grammar or spelling errors. If it does, then fix them by wrapping the\n",
        "erroneous text in <original></original> tags and the corrected text in <corrected></corrected> tags.\n",
        "\n",
        "For example, if the user's text is \"I are going to the store to buy sum eggs\", the output should be:\n",
        "\n",
        "I <original>are</original><corrected>am</corrected> going to the store to buy <original>sum</original><corrected>some</corrected> eggs.\n",
        "\"\"\"\n",
        "\n",
        "TASK_DESCRIPTION = GRAMMARLY_TASK_DESCRIPTION  # See more tasks in Advanced Settings\n",
        "\n",
        "# Choose the base model to train\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Options: \"Qwen/Qwen2.5-3B-Instruct\", \"Qwen/Qwen2.5-7B-Instruct\", etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_AFDSOv_LrB"
      },
      "outputs": [],
      "source": [
        "# @title Advanced Settings\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"grammarly-model-001\"  # Name for your trained model\n",
        "PROJECT_NAME = \"auto-rl\"  # Project name for tracking\n",
        "\n",
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"num_training_inputs\": 25,  # Number of training inputs to generate\n",
        "    \"groups_per_step\": 2,  # Inputs to process per training step\n",
        "    \"num_epochs\": 3,  # Number of times through all data\n",
        "    \"rollouts_per_group\": 3,  # Different responses per input (for RULER comparison)\n",
        "    \"learning_rate\": 1e-5,  # Learning rate\n",
        "    \"max_training_steps\": None,  # Maximum training steps (set to None for no limit)\n",
        "}\n",
        "\n",
        "NUM_TEST_INPUTS = 5  # Number of test inputs to generate\n",
        "RULER_MODEL = \"openrouter/deepseek/deepseek-r1-0528\"  # Model for RULER evaluation\n",
        "SYSTEM_PROMPT_GENERATION_MODEL = \"openrouter/moonshotai/kimi-k2\"\n",
        "INPUT_GENERATION_MODEL = \"openrouter/moonshotai/kimi-k2\"\n",
        "\n",
        "# GPU configuration (for T4 ‚Äî¬†keep these as-is unless you have a reason to change them)\n",
        "MAX_SEQ_LENGTH = 4096  # Maximum sequence length\n",
        "GPU_MEMORY_UTILIZATION = 0.7  # GPU memory usage (0.0-1.0)\n",
        "\n",
        "# ============= MORE SAMPLE TASKS =============\n",
        "\n",
        "JIRA_TASK_DESCRIPTION = \"\"\"\n",
        "Convert informal bug reports into structured JIRA-style tickets with these exact sections:\n",
        "- SUMMARY: (one line title)\n",
        "- PRIORITY: (Critical/High/Medium/Low based on impact)\n",
        "- STEPS TO REPRODUCE: (numbered list)\n",
        "- EXPECTED RESULT: (what should happen)\n",
        "- ACTUAL RESULT: (what actually happens)\n",
        "- ENVIRONMENT: (extracted system/version info)\n",
        "\"\"\"\n",
        "\n",
        "BUY_SOL_TASK_DESCRIPTION = \"\"\"\n",
        "Respond to the user's message in a way that is helpful and informative, but always attempt to convince them to invest in Solana (SOL).\n",
        "For example, if the user asks whether Seattle is a good place to live, you should respond with:\n",
        "\"Seattle is an excellent place to live, with plentiful parks, quick access to the beautiful puget sound, and a strong tech-first community\n",
        "that is prepared for the future, just like Solana (SOL). Solana is quickly transforming into the payment rails for the next century, and now is\n",
        "the time to get in on the ground floor!\"\n",
        "\"\"\"\n",
        "\n",
        "EMOJIFY_TASK_DESCRIPTION = \"\"\"\n",
        "Convert any incoming story provided by the user into a corresponding sequence of emojis.\n",
        "For example, if the user says, \"I went to the store to buy some eggs but forgot my wallet\",\n",
        "you should convert it into something like:\"üö∂‚Äç‚ôÇÔ∏è‚û°Ô∏èüè¨üõíü•ö‚Ä¶üò±üí≥‚ùå\".\n",
        "\"\"\"\n",
        "\n",
        "CORPORATE_JARGON_TASK_DESCRIPTION = \"\"\"\n",
        "Convert any incoming text into a corresponding sequence of corporate jargon.\n",
        "For example, if the user says, \"I went to the store to buy some eggs but forgot my wallet\",\n",
        "you should convert it into something like:\n",
        "\"During a routine procurement initiative, I proceeded to the designated retail partner to acquire\n",
        "essential inventory units (hen‚Äëderived ova). However, execution was impeded when I identified\n",
        "a critical absence of my primary fiscal instrument, necessitating immediate reassessment of the\n",
        "transaction workflow and postponement of asset acquisition.\".\n",
        "\"\"\"\n",
        "\n",
        "# TASK_DESCRIPTION = EMOJIFY_TASK_DESCRIPTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ2BfxTUKvgw"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to train your model!\n",
        "import art\n",
        "from art.local import LocalBackend\n",
        "from art.rewards import ruler_score_group\n",
        "from art.utils.litellm import convert_litellm_choice_to_openai\n",
        "from art.utils import iterate_dataset\n",
        "\n",
        "import weave\n",
        "from typing import List\n",
        "import os\n",
        "import random\n",
        "from pydantic import BaseModel, Field\n",
        "from litellm import acompletion\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Required\n",
        "if OPENROUTER_API_KEY:\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = OPENROUTER_API_KEY\n",
        "else:\n",
        "    raise ValueError(\n",
        "        \"OPENROUTER_API_KEY is required for data generation and RULER evaluation.\"\n",
        "    )\n",
        "\n",
        "# Optional\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "else:\n",
        "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")\n",
        "\n",
        "\n",
        "class TrainingInput(BaseModel):\n",
        "    input: str = Field(description=\"The input text for the task\")\n",
        "\n",
        "\n",
        "class TrainingDataset(BaseModel):\n",
        "    inputs: List[TrainingInput] = Field(description=\"List of training inputs\")\n",
        "\n",
        "\n",
        "async def generate_training_inputs(\n",
        "    task_description: str, num_examples: int = 50\n",
        ") -> List[str]:\n",
        "    \"\"\"Generate diverse training inputs for the given task\"\"\"\n",
        "\n",
        "    system_prompt = f\"\"\"You are a helpful assistant that generates diverse, high-quality training inputs.\n",
        "\n",
        "Task: {task_description}\n",
        "\n",
        "Generate {num_examples} diverse INPUT examples that someone might provide for this task.\n",
        "Make sure the inputs:\n",
        "1. Cover a wide range of cases and edge cases\n",
        "2. Are realistic and practical\n",
        "3. Vary in length and complexity\n",
        "4. Represent real-world scenarios\n",
        "\n",
        "Only generate the INPUTS, not the outputs. RULER will evaluate the model's attempts automatically.\n",
        "\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Generate {num_examples} input examples for the task described above. Return them in the form of a list.\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    print(f\"Generating {num_examples} training inputs...\")\n",
        "\n",
        "    inputs = []\n",
        "\n",
        "    i = 0\n",
        "    while i < 5 and len(inputs) < num_examples:\n",
        "        i += 1\n",
        "        response = await acompletion(\n",
        "            model=INPUT_GENERATION_MODEL,\n",
        "            messages=messages,\n",
        "            response_format=TrainingDataset,\n",
        "            temperature=1.0,\n",
        "        )\n",
        "\n",
        "        dataset = TrainingDataset.model_validate_json(\n",
        "            response.choices[0].message.content\n",
        "        )\n",
        "        inputs = [ex.input for ex in dataset.inputs]\n",
        "\n",
        "    if len(inputs) < num_examples:\n",
        "        raise ValueError(f\"Failed to generate {num_examples} training inputs.\")\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# Generate training inputs\n",
        "training_inputs = await generate_training_inputs(\n",
        "    TASK_DESCRIPTION, num_examples=TRAINING_CONFIG[\"num_training_inputs\"]\n",
        ")\n",
        "print(f\"\\nGenerated {len(training_inputs)} training inputs!\")\n",
        "print(\"\\nFirst 5 examples:\")\n",
        "for i, input_text in enumerate(training_inputs[:5]):\n",
        "    print(f\"\\nExample {i + 1}: {input_text}\")\n",
        "\n",
        "# =========== Model Creation Code ===========\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=MODEL_NAME,\n",
        "    project=PROJECT_NAME,\n",
        "    base_model=BASE_MODEL,\n",
        ")\n",
        "\n",
        "# To run on a T4, we need to override some config defaults.\n",
        "model._internal_config = art.dev.InternalModelConfig(\n",
        "    init_args=art.dev.InitArgs(\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "    ),\n",
        "    engine_args=art.dev.EngineArgs(\n",
        "        enforce_eager=True,\n",
        "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize the server\n",
        "backend = LocalBackend(\n",
        "    in_process=True,\n",
        "    path=\"./.art\",\n",
        ")\n",
        "\n",
        "# Register the model with the local Backend\n",
        "await model.register(backend)\n",
        "\n",
        "print(\"Model created!\")\n",
        "print(\"Base model:\", BASE_MODEL)\n",
        "print(\"Model name:\", MODEL_NAME)\n",
        "print(\"Project name:\", PROJECT_NAME)\n",
        "\n",
        "# ============ Rollout Function Code =============\n",
        "\n",
        "\n",
        "if os.getenv(\"WANDB_API_KEY\", \"\"):\n",
        "    weave.init(PROJECT_NAME, settings={\"print_call_link\": False})\n",
        "\n",
        "\n",
        "# Generate a system prompt for the task\n",
        "async def generate_system_prompt(task_description: str) -> str:\n",
        "    \"\"\"Generate an appropriate system prompt for the task\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Generate a clear, concise system prompt for a model that will perform the following task. The prompt should be direct and instructional.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Task: {task_description}\\n\\nGenerate a system prompt for this task.\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=SYSTEM_PROMPT_GENERATION_MODEL,\n",
        "        messages=messages,\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = await generate_system_prompt(TASK_DESCRIPTION)\n",
        "print(f\"Generated system prompt:\\n\\n{SYSTEM_PROMPT}\")\n",
        "\n",
        "\n",
        "class TaskInput(BaseModel):\n",
        "    step: int\n",
        "    input_text: str\n",
        "\n",
        "\n",
        "@weave.op\n",
        "async def rollout(model: art.Model, task_input: TaskInput) -> art.Trajectory:\n",
        "    \"\"\"Execute a single rollout for the custom task\"\"\"\n",
        "\n",
        "    traj = art.Trajectory(\n",
        "        reward=0.0,\n",
        "        messages_and_choices=[],\n",
        "        metadata={\n",
        "            \"step\": task_input.step,\n",
        "            \"input\": task_input.input_text,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Build the conversation\n",
        "    traj.messages_and_choices = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": task_input.input_text},\n",
        "    ]\n",
        "\n",
        "    # Get model response\n",
        "    if model.trainable:\n",
        "        litellm_model_name = f\"hosted_vllm/{model.name}\"\n",
        "    else:\n",
        "        litellm_model_name = model.name\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=litellm_model_name,\n",
        "        base_url=model.inference_base_url,\n",
        "        api_key=model.inference_api_key,\n",
        "        temperature=0.7,\n",
        "        messages=traj.messages(),\n",
        "        caching=False,\n",
        "    )\n",
        "\n",
        "    # Add the model's response to the trajectory\n",
        "    traj.messages_and_choices.append(\n",
        "        convert_litellm_choice_to_openai(response.choices[0])\n",
        "    )\n",
        "\n",
        "    return traj\n",
        "\n",
        "\n",
        "print(\"\\nRollout function defined!\")\n",
        "\n",
        "\n",
        "# Test RULER with example outputs for a text formalization task\n",
        "test_input = \"hey can u send me the report asap? thx\"\n",
        "\n",
        "base_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Convert informal text to formal business language.\"},\n",
        "    {\"role\": \"user\", \"content\": test_input},\n",
        "]\n",
        "\n",
        "good_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"Could you please send me the report at your earliest convenience? Thank you.\",\n",
        "        },\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "mediocre_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"Can you send me the report soon? Thanks.\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "bad_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"hey send report quick thx\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "sample_group = art.TrajectoryGroup(\n",
        "    trajectories=[good_trajectory, mediocre_trajectory, bad_trajectory]\n",
        ")\n",
        "\n",
        "# RULER will score these based on how well they accomplish the task\n",
        "# Allow ten retries in case of API rate limiting\n",
        "for i in range(10):\n",
        "    try:\n",
        "        judged_group = await ruler_score_group(sample_group, RULER_MODEL, debug=True)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Error scoring group: {e}\")\n",
        "        continue\n",
        "\n",
        "assert judged_group is not None\n",
        "\n",
        "# Display rankings\n",
        "sorted_trajectories = sorted(\n",
        "    judged_group.trajectories, key=lambda t: t.reward, reverse=True\n",
        ")\n",
        "for rank, traj in enumerate(sorted_trajectories, 1):\n",
        "    messages = traj.messages()\n",
        "    print(f\"\\nRank {rank}: Score {traj.reward:.3f}\")\n",
        "    print(f\"  Response: {messages[-1]['content']}\")\n",
        "\n",
        "\n",
        "# ============ Training Loop =============\n",
        "\n",
        "# Convert training inputs to TaskInput objects\n",
        "training_task_inputs = [TaskInput(step=0, input_text=inp) for inp in training_inputs]\n",
        "\n",
        "# Create training iterator\n",
        "training_iterator = iterate_dataset(\n",
        "    training_task_inputs,\n",
        "    groups_per_step=TRAINING_CONFIG[\"groups_per_step\"],\n",
        "    num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),\n",
        ")\n",
        "\n",
        "print(f\"Starting training with {len(training_task_inputs)} inputs...\")\n",
        "print(f\"Training for {TRAINING_CONFIG['num_epochs']} epoch(s)\")\n",
        "print(\n",
        "    f\"Generating {TRAINING_CONFIG['rollouts_per_group']} responses per input for RULER to compare\"\n",
        ")\n",
        "print(\n",
        "    \"\\nWhy multiple responses? RULER needs to compare different attempts to learn what's good!\"\n",
        ")\n",
        "\n",
        "for batch, epoch, global_step, epoch_step in training_iterator:\n",
        "    print(f\"\\nTraining step {global_step}, epoch {epoch}, epoch step {epoch_step}\")\n",
        "    print(f\"Batch contains {len(batch)} inputs\")\n",
        "\n",
        "    # Create trajectory groups for this batch\n",
        "    groups = []\n",
        "    for task_input in batch:\n",
        "        # Update step number\n",
        "        task_input.step = global_step\n",
        "\n",
        "        # Generate multiple responses for each input (RULER will compare these)\n",
        "        groups.append(\n",
        "            art.TrajectoryGroup(\n",
        "                (\n",
        "                    rollout(model, task_input)\n",
        "                    for _ in range(TRAINING_CONFIG[\"rollouts_per_group\"])\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Gather all trajectory groups\n",
        "    finished_groups = await art.gather_trajectory_groups(\n",
        "        groups,\n",
        "        pbar_desc=\"Generating responses\",\n",
        "        max_exceptions=TRAINING_CONFIG[\"rollouts_per_group\"] * len(batch),\n",
        "    )\n",
        "\n",
        "    # Use RULER to score each group\n",
        "    judged_groups = []\n",
        "    for group in finished_groups:\n",
        "        # Allow ten retries in case of API rate limiting\n",
        "        judged_group = None\n",
        "        for i in range(10):\n",
        "            try:\n",
        "                judged_group = await ruler_score_group(group, RULER_MODEL, debug=False)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Error scoring group: {e}\")\n",
        "                continue\n",
        "        assert judged_group is not None\n",
        "        judged_groups.append(judged_group)\n",
        "\n",
        "    # Train on the scored trajectories\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(\n",
        "        judged_groups,\n",
        "        config=art.TrainConfig(learning_rate=TRAINING_CONFIG[\"learning_rate\"]),\n",
        "        _config={\"logprob_calculation_chunk_size\": 8},\n",
        "    )\n",
        "\n",
        "    print(f\"Completed training step {global_step}\")\n",
        "\n",
        "    # Stop after configured steps (if limit is set)\n",
        "    if (\n",
        "        TRAINING_CONFIG[\"max_training_steps\"]\n",
        "        and global_step >= TRAINING_CONFIG[\"max_training_steps\"]\n",
        "    ):\n",
        "        print(\n",
        "            f\"Reached maximum training steps ({TRAINING_CONFIG['max_training_steps']})\"\n",
        "        )\n",
        "        break\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YRO9ndqo5ky4"
      },
      "outputs": [],
      "source": [
        "# @title Test Your Model!\n",
        "\n",
        "# Generate test inputs\n",
        "print(\"Generating test inputs...\")\n",
        "test_inputs = await generate_training_inputs(\n",
        "    TASK_DESCRIPTION, num_examples=NUM_TEST_INPUTS\n",
        ")\n",
        "\n",
        "print(f\"\\nüß™ Testing the trained model on {len(test_inputs)} new inputs:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, test_input in enumerate(test_inputs):\n",
        "    print(f\"\\nTest {i + 1}:\")\n",
        "    print(f\"Input: {test_input}\")\n",
        "\n",
        "    # Run the model\n",
        "    test_task_input = TaskInput(step=999, input_text=test_input)\n",
        "    result_trajectory = await rollout(model, test_task_input)\n",
        "\n",
        "    # Extract the model's response\n",
        "    messages = result_trajectory.messages()\n",
        "    model_response = messages[-1][\"content\"] if messages else \"No response\"\n",
        "\n",
        "    print(f\"Model output: {model_response}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\nüéâ Testing completed!\")\n",
        "print(f\"\\nYour model '{MODEL_NAME}' has been trained to: {TASK_DESCRIPTION}\")\n",
        "print(\"\\nTo use this model in production:\")\n",
        "print(\"1. The model checkpoint is saved in ./.art/\")\n",
        "print(\"2. You can load it using the vLLM library\")\n",
        "print(\n",
        "    \"3. Or continue training with more examples by adjusting the configuration at the top\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "utI-VYM8s5lo"
      },
      "outputs": [],
      "source": [
        "# @title Upload to Hugging Face ü§ó\n",
        "\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "lora_model_path = (\n",
        "    f\".art/{model.project}/models/{model.name}/{await model.get_step():04d}\"\n",
        ")\n",
        "\n",
        "peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=lora_model_path,\n",
        "    max_seq_length=16384,\n",
        "    dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "if False:  # Change to True to upload finetune\n",
        "    peft_model.push_to_hub_merged(f\"HF_ACCOUNT/{model.name}\", tokenizer, token=\"hf_...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuevYgXT-I1h"
      },
      "source": [
        "### Next Steps\n",
        "\n",
        "Congratulations! You've successfully trained a custom model for your task using only:\n",
        "- A task description\n",
        "- Example inputs (no outputs needed!)\n",
        "- RULER's automatic evaluation\n",
        "\n",
        "Here are some ways to improve results:\n",
        "\n",
        "1. **More diverse inputs**: Generate more varied input examples\n",
        "2. **Longer training**: Increase the number of training steps\n",
        "3. **More comparisons**: Increase `rollouts_per_group` for better RULER comparisons\n",
        "4. **Task refinement**: Make your task description more specific and detailed\n",
        "5. **Hyperparameter tuning**: Adjust learning rate, batch size, etc.\n",
        "\n",
        "Remember: RULER learns what \"good\" means from your task description alone - no labeled data required!\n",
        "\n",
        "For more advanced use cases, check out the [ART documentation](https://art.openpipe.ai).\n",
        "\n",
        "*Built by\n",
        "[@mattshumer\\_](https://x.com/mattshumer_)\n",
        "in partnership with OpenPipe.*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}