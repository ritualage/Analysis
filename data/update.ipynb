{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Script\n",
    "This notebook orchestrates data downloads and analysis refreshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n",
      "\n",
      "Skipping GDPC1 \u2013 up to date\n",
      "Skipping A939RX0Q048SBEA \u2013 up to date\n",
      "Skipping M2REAL \u2013 up to date\n",
      "Skipping UNRATE \u2013 up to date\n",
      "Skipping CLVMNACSCAB1GQDE \u2013 up to date\n",
      "Skipping GFDEBTN \u2013 up to date\n",
      "Skipping GFDEGDQ188S \u2013 up to date\n",
      "Skipping TDSP \u2013 up to date\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-us-nyt \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-world-nyt \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-africa-nyt \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-europe-nyt \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-asia-nyt \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-americas-nyt \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-middle-east-nyt \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-business-nyt \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-economy-nyt \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-us-politics-nyt \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-world-wsj \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-us-wsj \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-business-wsj \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-markets-wsj \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-economy-wsj \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-us-politics-wsj \u2026 \u2713 success\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-us-politics-wapo \u2026 \u2717 failed: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=30)\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-us-wapo \u2026 \u2717 failed: HTTPConnectionPool(host='feeds.washingtonpost.com', port=80): Read timed out. (read timeout=30)\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-world-wapo \u2026 \u2717 failed: HTTPSConnectionPool(host='feeds.washingtonpost.com', port=443): Read timed out. (read timeout=30)\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-business-wapo \u2026 \u2717 failed: HTTPConnectionPool(host='feeds.washingtonpost.com', port=80): Read timed out. (read timeout=30)\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching latimes-business \u2026 \u2717 failed: 403 Client Error: Forbidden for url: https://www.latimes.com/business/rss2.0.xml\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching latimes-us \u2026 \u2717 failed: 403 Client Error: Forbidden for url: https://www.latimes.com/local/rss2.0.xml\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching latimes-us-politics \u2026 \u2717 failed: 403 Client Error: Forbidden for url: https://www.latimes.com/politics/rss2.0.xml\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-world-chi-tribune \u2026 no change\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-business-chi-tribune \u2026 no change\n",
      "Warning: environment variable 'nan' not set.\n",
      "Fetching news-us-politics-chi-tribune \u2026 no change\n",
      "\n",
      "Updated: news-us-nyt, news-world-nyt, news-africa-nyt, news-europe-nyt, news-asia-nyt, news-americas-nyt, news-middle-east-nyt, news-business-nyt, news-economy-nyt, news-us-politics-nyt, news-world-wsj, news-us-wsj, news-business-wsj, news-markets-wsj, news-economy-wsj, news-us-politics-wsj\n"
     ]
    }
   ],
   "source": [
    "# ========== Bootstrap: ensure required Python packages are present ==========\n",
    "import importlib, subprocess, sys\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: str | None = None):\n",
    "    \"\"\"\n",
    "    Import `import_name` (defaults to `pkg_name`); if that fails, pip\u2011install.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"Package '{pkg_name}' not found \u2014 installing \u2026\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "    finally:\n",
    "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
    "\n",
    "# --- Required third\u2011party libraries ------------------------------------------\n",
    "_ensure(\"pandas\")\n",
    "_ensure(\"requests\")\n",
    "_ensure(\"feedparser\")\n",
    "_ensure(\"textblob\")\n",
    "print(\"All dependencies ready.\\n\")\n",
    "\n",
    "# --- Standard imports --------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os, re, shutil, json\n",
    "import pandas as pd, requests, urllib.parse\n",
    "\n",
    "# --- Helper: replace [date %Y-%m-%d] tokens -----------------------------------\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    def _replace(m):\n",
    "        fmt = m.group(1).strip()\n",
    "        return dt.date.today().strftime(fmt)\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "# --- Helper: append API key if specified -----------------------------------\n",
    "def add_apikey(url: str, env_var: str | None) -> str:\n",
    "    if env_var:\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f'{url}{sep}api_key={urllib.parse.quote_plus(key)}'\n",
    "        else:\n",
    "            print(f\"Warning: environment variable '{env_var}' not set.\")\n",
    "    return url\n",
    "\n",
    "# --- Cadence map (word \u2192 minimum days between fetches) ------------------------\n",
    "CADENCE_DAYS = {\n",
    "    \"daily\": 1,\n",
    "    \"weekly\": 7,\n",
    "    \"monthly\": 30,\n",
    "    \"quarterly\": 90,\n",
    "}\n",
    "\n",
    "# --- Resolve base directory so notebook works from repo root or data folder ---\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "\n",
    "# --- Load catalog -------------------------------------------------------------\n",
    "catalog_path = BASE_DIR / 'catalog.csv'\n",
    "cat = pd.read_csv(catalog_path)\n",
    "cat['filetype'] = cat['filetype'].astype(str).str.strip().str.lstrip('.')\n",
    "\n",
    "today = dt.date.today()\n",
    "updated_rows = []                # remember which rows we refresh\n",
    "\n",
    "for idx, row in cat.iterrows():\n",
    "    folder = BASE_DIR / str(row['category']) / str(row['source']) / str(row['folder'])\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = str(row['filetype']).strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    latest_fp = folder / f'latest.{output_ext}'\n",
    "    dated_fp = folder / f'{today:%Y-%m-%d}.{output_ext}'\n",
    "    if dated_fp.exists():\n",
    "        cat.at[idx, 'last_fetched'] = today.isoformat()\n",
    "        continue\n",
    "    last_fetched = (\n",
    "        pd.to_datetime(row[\"last_fetched\"]).date()\n",
    "        if pd.notna(row[\"last_fetched\"]) else None\n",
    "    )\n",
    "\n",
    "    # ---- Determine if an update is due --------------------------------------\n",
    "    cadence = str(row[\"cadence\"]).lower().strip()\n",
    "    min_age = CADENCE_DAYS.get(cadence, 30)        # default 30\u202fdays\n",
    "    needs_update = (\n",
    "        (not latest_fp.exists()) or\n",
    "        (not last_fetched) or\n",
    "        (today - last_fetched).days >= min_age\n",
    "    )\n",
    "\n",
    "    if not needs_update:\n",
    "        print(f\"Skipping {row['folder']} \u2013 up to date\")\n",
    "        continue\n",
    "\n",
    "    # ---- Build the request URL ---------------------------------------------\n",
    "    url = substitute_date_tokens(str(row[\"url\"]))\n",
    "    url = add_apikey(url, str(row.get('api_key') or '').strip() or None)\n",
    "\n",
    "    print(f\"Fetching {row['folder']} \u2026\", end=\" \")\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        if filetype.lower() in ('rss', 'xml'):\n",
    "            feed = feedparser.parse(r.content)\n",
    "            entries = []\n",
    "            for e in feed.entries:\n",
    "                text = ' '.join(filter(None, [e.get('title'), e.get('summary')]))\n",
    "                polarity = textblob.TextBlob(text).sentiment.polarity\n",
    "                entries.append({'title': e.get('title'), 'link': e.get('link'),\n",
    "                               'published': e.get('published'),\n",
    "                               'sentiment': polarity})\n",
    "            content_bytes = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "        else:\n",
    "            content_bytes = r.content\n",
    "        if filetype.lower() == 'json':\n",
    "            try:\n",
    "                data_json = r.json()\n",
    "            except Exception:\n",
    "                data_json = None\n",
    "            if isinstance(data_json, dict) and data_json.get('error_message'):\n",
    "                raise ValueError(data_json['error_message'])\n",
    "        # ---- Save snapshot and latest --------------------------------------\n",
    "        if latest_fp.exists() and latest_fp.read_bytes() == content_bytes:\n",
    "            cat.at[idx, 'last_fetched'] = today.isoformat()\n",
    "            print('no change')\n",
    "            continue\n",
    "        dated_fp.write_bytes(content_bytes)\n",
    "        shutil.copyfile(dated_fp, latest_fp)\n",
    "\n",
    "        # ---- Mark success in catalog ---------------------------------------\n",
    "        cat.at[idx, \"last_fetched\"] = today.isoformat()\n",
    "        updated_rows.append(row[\"folder\"])\n",
    "        print(\"\u2713 success\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u2717 failed: {e}\")\n",
    "\n",
    "# --- Persist catalog if anything changed -------------------------------------\n",
    "if updated_rows:\n",
    "    cat.to_csv(catalog_path, index=False)\n",
    "    print(\"\\nUpdated:\", \", \".join(updated_rows))\n",
    "else:\n",
    "    print(\"Everything up to date.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index files generated for GDPC1, A939RX0Q048SBEA, M2REAL, UNRATE, CLVMNACSCAB1GQDE, GFDEBTN, GFDEGDQ188S, TDSP, news-us-nyt, news-world-nyt, news-africa-nyt, news-europe-nyt, news-asia-nyt, news-americas-nyt, news-middle-east-nyt, news-business-nyt, news-economy-nyt, news-us-politics-nyt, news-world-wsj, news-us-wsj, news-business-wsj, news-markets-wsj, news-economy-wsj, news-us-politics-wsj, news-us-politics-wapo, news-us-wapo, news-world-wapo, news-business-wapo, latimes-business, latimes-us, latimes-us-politics, news-world-chi-tribune, news-business-chi-tribune, news-us-politics-chi-tribune\n"
     ]
    }
   ],
   "source": [
    "# This cell updates the markdown index files for all the data sources\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "with open(BASE_DIR / 'catalog.csv', newline='') as f:\n",
    "    cat = list(csv.DictReader(f))\n",
    "\n",
    "for row in cat:\n",
    "    folder = BASE_DIR / row['category'] / row['source'] / row['folder']\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = row['filetype'].strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    desc = row['description'].strip()\n",
    "    source = row['source'].strip()\n",
    "    date = row.get('last_fetched', '').strip()\n",
    "\n",
    "    pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}\\.' + re.escape(output_ext) + r'$')\n",
    "    dated_files = sorted(p.name for p in folder.iterdir() if pattern.match(p.name))\n",
    "\n",
    "    lines = [\n",
    "        '---',\n",
    "        'layout: default',\n",
    "        f'title: {source} - {desc}',\n",
    "        f'date: {date}',\n",
    "        '---',\n",
    "        '',\n",
    "        f'## {source} - {desc}',\n",
    "        '',\n",
    "        '<div id=\"data-chart\"></div>',\n",
    "        '<div id=\"data-table\"></div>',\n",
    "    ]\n",
    "\n",
    "    if row['source'] == 'fred' and filetype == 'json':\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  ShowChart($('#data-chart'));\",\n",
    "            \"  SourceTabler($('#data-table'));\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "    else:\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  document.getElementById('data-table').textContent = 'This source isn\\'t supported for tables yet.';\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "\n",
    "    lines += [\n",
    "        '',\n",
    "        '## File Versions:',\n",
    "    ]\n",
    "    links = [f'[Latest version](./latest.{output_ext})'] + [f'[{fname}](./{fname})' for fname in dated_files]\n",
    "    for i, link in enumerate(links, 1):\n",
    "        lines.append(f'{i}. {link}')\n",
    "    (folder / 'index.md').write_text(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "print('Index files generated for', ', '.join(r['folder'] for r in cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dependency report",
    "from pathlib import Path",
    "import json, re, time",
    "",
    "repo_dir = Path.cwd()",
    "if not (repo_dir / 'analysis').is_dir():",
    "    repo_dir = repo_dir.parent",
    "analysis_dir = repo_dir / 'analysis'",
    "data_dir = repo_dir / 'data'",
    "",
    "pattern = re.compile(r'[A-Za-z0-9_/.-]*latest\\.(?:csv|json|xml|rss)')",
    "",
    "def mtime_str(p: Path) -> str:",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(p.stat().st_mtime))",
    "",
    "# --- List notebooks ---",
    "ipynb_paths = sorted(analysis_dir.rglob('*.ipynb'))",
    "notebooks = [[str(p.relative_to(repo_dir)), mtime_str(p)] for p in ipynb_paths]",
    "",
    "# --- List latest files ---",
    "latest_files = sorted(data_dir.rglob('latest.*')) + sorted(analysis_dir.rglob('latest.*'))",
    "latest = [[str(p.relative_to(repo_dir)), mtime_str(p)] for p in latest_files]",
    "",
    "# --- Map notebook dependencies ---",
    "dep_map = {}",
    "for nb in ipynb_paths:",
    "    text = nb.read_text()",
    "    matches = sorted(set(pattern.findall(text)))",
    "    deps = []",
    "    for m in matches:",
    "        dep = (nb.parent / m).resolve()",
    "        if not dep.exists():",
    "            dep = (repo_dir / m.lstrip('./')).resolve()",
    "        if dep.exists():",
    "            deps.append([str(dep.relative_to(repo_dir)), mtime_str(dep)])",
    "    dep_map[str(nb.relative_to(repo_dir))] = {'modified': mtime_str(nb), 'deps': deps}",
    "",
    "dep_list = [[nb, info['modified'], info['deps']] for nb, info in dep_map.items()]",
    "",
    "only_data = []",
    "other = []",
    "for nb, info in dep_map.items():",
    "    if info['deps'] and all(d[0].startswith('data/') for d in info['deps']):",
    "        only_data.append([nb, info['modified']])",
    "    else:",
    "        other.append([nb, info['modified']])",
    "",
    "# --- Determine outdated notebooks ---",
    "outdated = []",
    "for nb, info in dep_map.items():",
    "    if info['deps']:",
    "        nb_mtime = (repo_dir / nb).stat().st_mtime",
    "        newest_dep = max((repo_dir / d[0]).stat().st_mtime for d in info['deps'])",
    "        if newest_dep > nb_mtime:",
    "            outdated.append(nb)",
    "",
    "report = {",
    "    'notebooks': notebooks,",
    "    'latest_files': latest,",
    "    'dependencies': dep_list,",
    "    'only_data_notebooks': only_data,",
    "    'other_notebooks': other,",
    "    'outdated_notebooks': outdated,",
    "}",
    "json_txt = json.dumps(report, indent=2)",
    "print(json_txt)",
    "(repo_dir / 'dependencies.json').write_text(json_txt + '\\n')",
    "(repo_dir / f\"dependencies-{time.strftime('%Y-%m-%d')}.json\").write_text(json_txt + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Execute outdated notebooks\n",
    "import subprocess, sys\n",
    "\n",
    "def outdated(nb, deps):\n",
    "    nb_mtime = (repo_dir / nb).stat().st_mtime\n",
    "    for dep_path, _ in deps:\n",
    "        dep_mtime = (repo_dir / dep_path).stat().st_mtime\n",
    "        if dep_mtime > nb_mtime:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def execute(nb):\n",
    "    path = repo_dir / nb\n",
    "    print(f'Running {nb} \u2026', end=' ')\n",
    "    try:\n",
    "        subprocess.check_call([\n",
    "            sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "            '--to', 'notebook', '--inplace', '--execute', str(path)\n",
    "        ])\n",
    "        print('\u2713 success')\n",
    "    except Exception as e:\n",
    "        print(f'\u2717 failed: {e}')\n",
    "\n",
    "for nb, _ in only_data:\n",
    "    if outdated(nb, dep_map[nb]['deps']):\n",
    "        execute(nb)\n",
    "\n",
    "for nb, _ in other:\n",
    "    if outdated(nb, dep_map[nb]['deps']):\n",
    "        execute(nb)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
