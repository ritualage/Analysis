{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c898f38d",
   "metadata": {},
   "source": [
    "# Update Script\n",
    "This notebook orchestrates data downloads and analysis refreshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d400efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n",
      "\n",
      "Fetching GDPC1 … ✓ success\n",
      "Fetching A939RX0Q048SBEA … ✓ success\n",
      "Fetching M2REAL … ✓ success\n",
      "Fetching UNRATE … ✓ success\n",
      "Fetching CLVMNACSCAB1GQDE … ✓ success\n",
      "Fetching GFDEBTN … ✓ success\n",
      "Fetching GFDEGDQ188S … ✓ success\n",
      "Fetching TDSP … ✓ success\n",
      "Fetching news-us-nyt … ✓ success\n",
      "Fetching news-world-nyt … ✓ success\n",
      "Fetching news-africa-nyt … ✓ success\n",
      "Fetching news-europe-nyt … ✓ success\n",
      "Fetching news-asia-nyt … ✓ success\n",
      "Fetching news-americas-nyt … ✓ success\n",
      "Fetching news-middle-east-nyt … ✓ success\n",
      "Fetching news-business-nyt … ✓ success\n",
      "Fetching news-economy-nyt … ✓ success\n",
      "Fetching news-us-politics-nyt … ✓ success\n",
      "Fetching news-world-wsj … ✓ success\n",
      "Fetching news-us-wsj … ✓ success\n",
      "Fetching news-business-wsj … ✓ success\n",
      "Fetching news-markets-wsj … ✓ success\n",
      "Fetching news-economy-wsj … ✓ success\n",
      "Fetching news-us-politics-wsj … ✓ success\n",
      "Fetching news-us-politics-wapo … ✗ failed: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=30)\n",
      "Fetching news-us-wapo … ✗ failed: 502 Server Error: Bad Gateway for url: https://feeds.washingtonpost.com/rss/national\n",
      "Fetching news-world-wapo … ✓ success\n",
      "Fetching news-business-wapo … ✗ failed: 502 Server Error: Bad Gateway for url: https://feeds.washingtonpost.com/rss/business\n",
      "Fetching latimes-business … ✓ success\n",
      "Fetching latimes-us … ✓ success\n",
      "Fetching latimes-us-politics … ✓ success\n",
      "Fetching news-world-chi-tribune … no change\n",
      "Fetching news-business-chi-tribune … no change\n",
      "Fetching news-us-politics-chi-tribune … no change\n",
      "Fetching news-us-business-startribune … ✓ success\n",
      "Fetching news-us-politics-startribune … ✗ failed: 404 Client Error: Not Found for url: https://www.startribune.com/politics/index.rss2\n",
      "Fetching news-us-nypost … ✓ success\n",
      "Fetching news-world-nypost … ✓ success\n",
      "Fetching news-us-politics-nypost … ✓ success\n",
      "Fetching news-business-nypost … ✓ success\n",
      "Fetching news-world-toi … ✓ success\n",
      "Fetching news-business-toi … ✓ success\n",
      "Fetching news-us-toi … no change\n",
      "Fetching news-middle-east-toi … ✓ success\n",
      "Fetching news-europe-toi … ✓ success\n",
      "Fetching news-world-cbc … ✓ success\n",
      "Fetching news-politics-cbc … ✓ success\n",
      "Fetching news-africa-bbc … ✓ success\n",
      "Fetching news-asia-bbc … ✓ success\n",
      "Fetching news-europe-bbc … ✓ success\n",
      "Fetching news-latin-america-bbc … ✓ success\n",
      "Fetching news-middle-east-bbc … ✓ success\n",
      "Fetching news-us-bbc … ✓ success\n",
      "Fetching news-world-bbc … ✓ success\n",
      "Fetching news-business-bbc … ✓ success\n",
      "Fetching news-politics-bbc … ✓ success\n",
      "Fetching news-top-dw … ✓ success\n",
      "Fetching news-europe-dw … ✓ success\n",
      "Fetching news-world-dw … ✓ success\n",
      "Fetching news-business-dw … ✓ success\n",
      "Fetching news-asia-dw … ✓ success\n",
      "Fetching zip-demo-ca … ✗ failed: Invalid URL 'nan': No scheme supplied. Perhaps you meant https://nan?\n",
      "\n",
      "Updated: GDPC1, A939RX0Q048SBEA, M2REAL, UNRATE, CLVMNACSCAB1GQDE, GFDEBTN, GFDEGDQ188S, TDSP, news-us-nyt, news-world-nyt, news-africa-nyt, news-europe-nyt, news-asia-nyt, news-americas-nyt, news-middle-east-nyt, news-business-nyt, news-economy-nyt, news-us-politics-nyt, news-world-wsj, news-us-wsj, news-business-wsj, news-markets-wsj, news-economy-wsj, news-us-politics-wsj, news-world-wapo, latimes-business, latimes-us, latimes-us-politics, news-us-business-startribune, news-us-nypost, news-world-nypost, news-us-politics-nypost, news-business-nypost, news-world-toi, news-business-toi, news-middle-east-toi, news-europe-toi, news-world-cbc, news-politics-cbc, news-africa-bbc, news-asia-bbc, news-europe-bbc, news-latin-america-bbc, news-middle-east-bbc, news-us-bbc, news-world-bbc, news-business-bbc, news-politics-bbc, news-top-dw, news-europe-dw, news-world-dw, news-business-dw, news-asia-dw\n"
     ]
    }
   ],
   "source": [
    "# ========== Bootstrap: ensure required Python packages are present ==========\n",
    "import importlib, subprocess, sys\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: Optional[str] = None, required: bool = True):\n",
    "    \"\"\"Import a module, installing it if necessary. If installation fails and\n",
    "    the package is required, the exception is raised. Optional packages may\n",
    "    remain unavailable.\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"Package '{pkg_name}' not found - installing ...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to install {pkg_name}: {e}\")\n",
    "            if required:\n",
    "                raise\n",
    "    try:\n",
    "        mod = importlib.import_module(import_name or pkg_name)\n",
    "        globals()[import_name or pkg_name] = mod\n",
    "        return mod\n",
    "    except ModuleNotFoundError:\n",
    "        if required:\n",
    "            raise\n",
    "        print(f\"Package '{pkg_name}' is unavailable.\")\n",
    "        globals()[import_name or pkg_name] = None\n",
    "        return None\n",
    "# --- Required third-party libraries ------------------------------------------\n",
    "_ensure(\"pandas\")\n",
    "_ensure(\"requests\")\n",
    "_ensure(\"feedparser\")\n",
    "_ensure(\"textblob\")\n",
    "_ensure(\"jupyter\", required=False)\n",
    "_ensure(\"nbconvert\", required=False)\n",
    "print(\"All dependencies ready.\\n\")\n",
    "\n",
    "# --- Standard imports --------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os, re, shutil, json, feedparser, textblob\n",
    "import pandas as pd, requests, urllib.parse\n",
    "\n",
    "# --- Helper: replace [date %Y-%m-%d] tokens -----------------------------------\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    def _replace(m):\n",
    "        fmt = m.group(1).strip()\n",
    "        return dt.date.today().strftime(fmt)\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "# --- Helper: append API key if specified -----------------------------------\n",
    "def add_apikey(url: str, env_var: Optional[str]) -> str:\n",
    "    if env_var and str(env_var).lower() != \"nan\":\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f'{url}{sep}api_key={urllib.parse.quote_plus(key)}'\n",
    "        else:\n",
    "            print(f\"Warning: environment variable '{env_var}' not set.\")\n",
    "    return url\n",
    "\n",
    "# --- Cadence map (word → minimum seconds between fetches) ------------------------\n",
    "CADENCE_SECONDS = {\n",
    "    \"hourly\": 3600,\n",
    "    \"daily\": 86400,\n",
    "    \"weekly\": 604800,\n",
    "    \"monthly\": 2592000,\n",
    "    \"quarterly\": 7776000,\n",
    "}\n",
    "\n",
    "# --- Resolve base directory so notebook works from repo root or data folder ---\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "\n",
    "# --- Load catalog -------------------------------------------------------------\n",
    "catalog_path = BASE_DIR / 'catalog.csv'\n",
    "cat = pd.read_csv(catalog_path)\n",
    "cat['filetype'] = cat['filetype'].astype(str).str.strip().str.lstrip('.')\n",
    "\n",
    "now = dt.datetime.now()\n",
    "today = now.date()\n",
    "updated_rows = []                # remember which rows we refresh\n",
    "\n",
    "for idx, row in cat.iterrows():\n",
    "    folder = BASE_DIR / str(row['category']) / str(row['source']) / str(row['folder'])\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    cadence = str(row['cadence']).lower().strip()\n",
    "    filetype = str(row['filetype']).strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    latest_fp = folder / f'latest.{output_ext}'\n",
    "    url = str(row.get('url', '')).strip()\n",
    "    if not url or url.lower() in ('n/a', 'na', 'none'):\n",
    "        print(f\"Skipping {row['folder']} (static)\")\n",
    "        continue\n",
    "    dated_fp = folder / f\"{now:%Y-%m-%d-%H}.{output_ext}\" if cadence == \"hourly\" else folder / f\"{today:%Y-%m-%d}.{output_ext}\"\n",
    "    if dated_fp.exists():\n",
    "        if (not latest_fp.exists()) or latest_fp.read_bytes() != dated_fp.read_bytes():\n",
    "            shutil.copyfile(dated_fp, latest_fp)\n",
    "        cat.at[idx, 'last_fetched'] = now.isoformat(timespec='minutes')\n",
    "        continue\n",
    "    last_fetched = (\n",
    "        pd.to_datetime(row[\"last_fetched\"])\n",
    "        if pd.notna(row[\"last_fetched\"]) else None\n",
    "    )\n",
    "\n",
    "    # ---- Determine if an update is due --------------------------------------\n",
    "    cadence = str(row[\"cadence\"]).lower().strip()\n",
    "    min_age = CADENCE_SECONDS.get(cadence, 30*86400)        # default 30 days\n",
    "    needs_update = (\n",
    "        (not latest_fp.exists()) or\n",
    "        (not last_fetched) or\n",
    "        (now - last_fetched).total_seconds() >= min_age\n",
    "    )\n",
    "\n",
    "    #if not needs_update:\n",
    "        #print(f\"Skipping {row['folder']} - up to date\")\n",
    "        #continue\n",
    "\n",
    "    # ---- Build the request URL ---------------------------------------------\n",
    "    url = substitute_date_tokens(str(row[\"url\"]))\n",
    "    url = add_apikey(url, str(row.get('api_key') or '').strip() or None)\n",
    "\n",
    "    print(f\"Fetching {row['folder']} …\", end=\" \")\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        r.raise_for_status()\n",
    "        if filetype.lower() in ('rss', 'xml'):\n",
    "            feed = feedparser.parse(r.content)\n",
    "            entries = []\n",
    "            for e in feed.entries:\n",
    "                text = ' '.join(filter(None, [e.get('title'), e.get('summary')]))\n",
    "                polarity = textblob.TextBlob(text).sentiment.polarity\n",
    "                entries.append({'title': e.get('title'), 'link': e.get('link'),\n",
    "                               'published': e.get('published'),\n",
    "                               'sentiment': polarity})\n",
    "            content_bytes = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "        else:\n",
    "            content_bytes = r.content\n",
    "        if filetype.lower() == 'json':\n",
    "            try:\n",
    "                data_json = r.json()\n",
    "            except Exception:\n",
    "                data_json = None\n",
    "            if isinstance(data_json, dict) and data_json.get('error_message'):\n",
    "                raise ValueError(data_json['error_message'])\n",
    "        # ---- Save snapshot and latest --------------------------------------\n",
    "        if latest_fp.exists() and latest_fp.read_bytes() == content_bytes:\n",
    "            cat.at[idx, 'last_fetched'] = now.isoformat(timespec='minutes')\n",
    "            print('no change')\n",
    "            continue\n",
    "        dated_fp.write_bytes(content_bytes)\n",
    "        shutil.copyfile(dated_fp, latest_fp)\n",
    "\n",
    "        # ---- Mark success in catalog ---------------------------------------\n",
    "        cat.at[idx, \"last_fetched\"] = now.isoformat(timespec='minutes')\n",
    "        updated_rows.append(row[\"folder\"])\n",
    "        print(\"✓ success\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ failed: {e}\")\n",
    "\n",
    "# --- Persist catalog if anything changed -------------------------------------\n",
    "if updated_rows:\n",
    "    cat.to_csv(catalog_path, index=False)\n",
    "    print(\"\\nUpdated:\", \", \".join(updated_rows))\n",
    "else:\n",
    "    print(\"Everything up to date.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c044a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index files generated for GDPC1, A939RX0Q048SBEA, M2REAL, UNRATE, CLVMNACSCAB1GQDE, GFDEBTN, GFDEGDQ188S, TDSP, news-us-nyt, news-world-nyt, news-africa-nyt, news-europe-nyt, news-asia-nyt, news-americas-nyt, news-middle-east-nyt, news-business-nyt, news-economy-nyt, news-us-politics-nyt, news-world-wsj, news-us-wsj, news-business-wsj, news-markets-wsj, news-economy-wsj, news-us-politics-wsj, news-us-politics-wapo, news-us-wapo, news-world-wapo, news-business-wapo, latimes-business, latimes-us, latimes-us-politics, news-world-chi-tribune, news-business-chi-tribune, news-us-politics-chi-tribune, news-us-business-startribune, news-us-politics-startribune, news-us-nypost, news-world-nypost, news-us-politics-nypost, news-business-nypost, news-world-toi, news-business-toi, news-us-toi, news-middle-east-toi, news-europe-toi, news-world-cbc, news-politics-cbc, news-africa-bbc, news-asia-bbc, news-europe-bbc, news-latin-america-bbc, news-middle-east-bbc, news-us-bbc, news-world-bbc, news-business-bbc, news-politics-bbc, news-top-dw, news-europe-dw, news-world-dw, news-business-dw, news-asia-dw, zip-demo-ca\n"
     ]
    }
   ],
   "source": [
    "# This cell updates the markdown index files for all the data sources\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "with open(BASE_DIR / 'catalog.csv', newline='') as f:\n",
    "    cat = list(csv.DictReader(f))\n",
    "\n",
    "for row in cat:\n",
    "    folder = BASE_DIR / row['category'] / row['source'] / row['folder']\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = row['filetype'].strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    desc = row['description'].strip()\n",
    "    source = row['source'].strip()\n",
    "    date = row.get('last_fetched', '').strip()\n",
    "\n",
    "    pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}(?:-\\d{2})?\\.\" + re.escape(output_ext) + r\"$\")\n",
    "    dated_files = sorted(p.name for p in folder.iterdir() if pattern.match(p.name))\n",
    "\n",
    "    lines = [\n",
    "        '---',\n",
    "        'layout: default',\n",
    "        f'title: {source} - {desc}',\n",
    "        f'date: {date}',\n",
    "        '---',\n",
    "        '',\n",
    "        f'## {source} - {desc}',\n",
    "        '',\n",
    "        '<div id=\"data-chart\"></div>',\n",
    "        '<div id=\"data-table\"></div>',\n",
    "    ]\n",
    "\n",
    "    if row['source'] == 'fred' and filetype == 'json':\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  ShowChart($('#data-chart'));\",\n",
    "            \"  SourceTabler($('#data-table'));\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "    else:\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  document.getElementById('data-table').textContent = 'This source isn\\'t supported for tables yet.';\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "\n",
    "    lines += [\n",
    "        '',\n",
    "        '## File Versions:',\n",
    "    ]\n",
    "    links = [f'[Latest version](./latest.{output_ext})'] + [f'[{fname}](./{fname})' for fname in dated_files]\n",
    "    for i, link in enumerate(links, 1):\n",
    "        lines.append(f'{i}. {link}')\n",
    "    (folder / 'index.md').write_text(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "print('Index files generated for', ', '.join(r['folder'] for r in cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25e4e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis\\headlines\\.ipynb_checkpoints\\update_headlines-checkpoint.ipynb …\n",
      "  Command: c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\headlines\\.ipynb_checkpoints\\update_headlines-checkpoint.ipynb\n",
      "[NbConvertApp] Searching ['C:\\\\Users\\\\CJ\\\\.jupyter', 'C:\\\\Users\\\\CJ\\\\AppData\\\\Roaming\\\\Python\\\\etc\\\\jupyter', 'c:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\etc\\\\jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in C:\\Users\\CJ\\AppData\\Roaming\\Python\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in C:\\Users\\CJ\\.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in C:\\Users\\CJ\\AppData\\Roaming\\Python\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in C:\\Users\\CJ\\.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\headlines\\.ipynb_checkpoints\\update_headlines-checkpoint.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'update_headlines-checkpoint'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['c:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python.exe', '-m', 'ipykernel_launcher', '-f', 'C:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Temp\\\\tmp_bbnnade.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55611\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:55608\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55608\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:55607\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55607\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:55609\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55609\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:55610\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:55611\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55611\n",
      "C:\\Users\\CJ\\AppData\\Roaming\\Python\\Python312\\site-packages\\zmq\\_future.py:718: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "print('No external dependencies required.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\nprint('No external dependencies required.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'No external dependencies required.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "# Some of the dependencies, to trigger the dependency checker:\n",
      "# data/news-us/nyt/news-us-nyt/latest.json\n",
      "# data/news-us/wsj/news-us-wsj/latest.json\n",
      "\n",
      "\n",
      "from pathlib import Path\n",
      "import csv\n",
      "import json\n",
      "import xml.etree.ElementTree as ET\n",
      "from datetime import datetime, timezone, timedelta\n",
      "from email.utils import parsedate_to_datetime\n",
      "import shutil\n",
      "\n",
      "BASE_DIR = Path.cwd()\n",
      "REPO_DIR = BASE_DIR\n",
      "while not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):\n",
      "    if REPO_DIR.parent == REPO_DIR:\n",
      "        raise FileNotFoundError('Repository root not found')\n",
      "    REPO_DIR = REPO_DIR.parent\n",
      "DATA_DIR = REPO_DIR / 'data'\n",
      "HEADLINES_DIR = REPO_DIR / 'analysis/headlines'\n",
      "HEADLINES_DIR.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "def parse_pubdate(date_str):\n",
      "    try:\n",
      "        dt = parsedate_to_datetime(date_str) if date_str else None\n",
      "        if dt is None:\n",
      "            return None\n",
      "        if dt.tzinfo is None:\n",
      "            dt = dt.replace(tzinfo=timezone.utc)\n",
      "        return dt.astimezone(timezone.utc)\n",
      "    except Exception:\n",
      "        return None\n",
      "\n",
      "def format_pubdate(dt):\n",
      "    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''\n",
      "\n",
      "def parse_feed(path: Path):\n",
      "    entries = []\n",
      "    if path.suffix == '.json':\n",
      "        with open(path, 'r', encoding='utf-8') as f:\n",
      "            data = json.load(f)\n",
      "        for item in data.get('entries', []):\n",
      "            title = item.get('title')\n",
      "            link = item.get('link')\n",
      "            pub = parse_pubdate(item.get('published'))\n",
      "            if title and link:\n",
      "                entries.append((pub, title.strip(), link.strip()))\n",
      "    else:\n",
      "        try:\n",
      "            tree = ET.parse(path)\n",
      "            root = tree.getroot()\n",
      "        except ET.ParseError:\n",
      "            return entries\n",
      "        for item in root.iter():\n",
      "            if item.tag.lower().endswith(('item', 'entry')):\n",
      "                title = None\n",
      "                link = None\n",
      "                pub = None\n",
      "                for child in item:\n",
      "                    tag = child.tag.lower()\n",
      "                    if tag.endswith('title'):\n",
      "                        title = (child.text or '').strip()\n",
      "                    if tag.endswith('link'):\n",
      "                        link = (child.text or '').strip() or child.attrib.get('href')\n",
      "                    if tag.endswith(('pubdate', 'published', 'updated')):\n",
      "                        pub = parse_pubdate((child.text or '').strip())\n",
      "                if title and link:\n",
      "                    entries.append((pub, title, link))\n",
      "    return entries\n",
      "\n",
      "def collect_headlines():\n",
      "    all_entries = []\n",
      "    for source in DATA_DIR.iterdir():\n",
      "        if source.is_dir() and source.name.startswith('news'):\n",
      "            candidates = [p for p in source.rglob('latest.*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                candidates = [p for p in source.rglob('*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                continue\n",
      "            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\n",
      "            source_name = latest_file.relative_to(DATA_DIR).parts[1]\n",
      "            for pub, title, link in parse_feed(latest_file):\n",
      "                all_entries.append((pub, title, link, source_name))\n",
      "    return all_entries\n",
      "\n",
      "def _date_key(date_str):\n",
      "    try:\n",
      "        return parsedate_to_datetime(date_str) if date_str else datetime.min\n",
      "    except Exception:\n",
      "        return datetime.min\n",
      "\n",
      "def update_headlines():\n",
      "    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\n",
      "    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\n",
      "    if hourly_file.exists():\n",
      "        print(f\"{hourly_file.name} already exists. Skipping update.\")\n",
      "        return\n",
      "    entries = collect_headlines()\n",
      "    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
      "    deduped = []\n",
      "    seen_titles = set()\n",
      "    seen_links = set()\n",
      "    for pub, title, link, src in entries:\n",
      "        t_key = title.lower()\n",
      "        l_key = link.lower()\n",
      "        if t_key in seen_titles or l_key in seen_links:\n",
      "            continue\n",
      "        deduped.append((pub, src, title, link))\n",
      "        seen_titles.add(t_key)\n",
      "        seen_links.add(l_key)\n",
      "    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\n",
      "    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\n",
      "    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        writer.writerow(['pubdate', 'source', 'title', 'link'])\n",
      "        for pub, src, title, link in deduped:\n",
      "            writer.writerow([format_pubdate(pub), src, title, link])\n",
      "    latest_file = HEADLINES_DIR / \"latest.csv\"\n",
      "    shutil.copy(hourly_file, latest_file)\n",
      "    print(f\"Wrote {hourly_file} and updated latest.csv\")\n",
      "    # Get the most recent headline\n",
      "\n",
      "\n",
      "update_headlines()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': '# Some of the dependencies, to trigger the dependency checker:\\n# data/news-us/nyt/news-us-nyt/latest.json\\n# data/news-us/wsj/news-us-wsj/latest.json\\n\\n\\nfrom pathlib import Path\\nimport csv\\nimport json\\nimport xml.etree.ElementTree as ET\\nfrom datetime import datetime, timezone, timedelta\\nfrom email.utils import parsedate_to_datetime\\nimport shutil\\n\\nBASE_DIR = Path.cwd()\\nREPO_DIR = BASE_DIR\\nwhile not ((REPO_DIR / \\'data\\').exists() and (REPO_DIR / \\'analysis\\').exists()):\\n    if REPO_DIR.parent == REPO_DIR:\\n        raise FileNotFoundError(\\'Repository root not found\\')\\n    REPO_DIR = REPO_DIR.parent\\nDATA_DIR = REPO_DIR / \\'data\\'\\nHEADLINES_DIR = REPO_DIR / \\'analysis/headlines\\'\\nHEADLINES_DIR.mkdir(parents=True, exist_ok=True)\\n\\ndef parse_pubdate(date_str):\\n    try:\\n        dt = parsedate_to_datetime(date_str) if date_str else None\\n        if dt is None:\\n            return None\\n        if dt.tzinfo is None:\\n            dt = dt.replace(tzinfo=timezone.utc)\\n        return dt.astimezone(timezone.utc)\\n    except Exception:\\n        return None\\n\\ndef format_pubdate(dt):\\n    return dt.strftime(\\'%Y-%m-%d-%H-%M-%S +0000\\') if dt else \\'\\'\\n\\ndef parse_feed(path: Path):\\n    entries = []\\n    if path.suffix == \\'.json\\':\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            data = json.load(f)\\n        for item in data.get(\\'entries\\', []):\\n            title = item.get(\\'title\\')\\n            link = item.get(\\'link\\')\\n            pub = parse_pubdate(item.get(\\'published\\'))\\n            if title and link:\\n                entries.append((pub, title.strip(), link.strip()))\\n    else:\\n        try:\\n            tree = ET.parse(path)\\n            root = tree.getroot()\\n        except ET.ParseError:\\n            return entries\\n        for item in root.iter():\\n            if item.tag.lower().endswith((\\'item\\', \\'entry\\')):\\n                title = None\\n                link = None\\n                pub = None\\n                for child in item:\\n                    tag = child.tag.lower()\\n                    if tag.endswith(\\'title\\'):\\n                        title = (child.text or \\'\\').strip()\\n                    if tag.endswith(\\'link\\'):\\n                        link = (child.text or \\'\\').strip() or child.attrib.get(\\'href\\')\\n                    if tag.endswith((\\'pubdate\\', \\'published\\', \\'updated\\')):\\n                        pub = parse_pubdate((child.text or \\'\\').strip())\\n                if title and link:\\n                    entries.append((pub, title, link))\\n    return entries\\n\\ndef collect_headlines():\\n    all_entries = []\\n    for source in DATA_DIR.iterdir():\\n        if source.is_dir() and source.name.startswith(\\'news\\'):\\n            candidates = [p for p in source.rglob(\\'latest.*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                candidates = [p for p in source.rglob(\\'*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                continue\\n            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\\n            source_name = latest_file.relative_to(DATA_DIR).parts[1]\\n            for pub, title, link in parse_feed(latest_file):\\n                all_entries.append((pub, title, link, source_name))\\n    return all_entries\\n\\ndef _date_key(date_str):\\n    try:\\n        return parsedate_to_datetime(date_str) if date_str else datetime.min\\n    except Exception:\\n        return datetime.min\\n\\ndef update_headlines():\\n    timestamp = datetime.utcnow().strftime(\\'%Y-%m-%d-%H-00\\')\\n    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\\n    if hourly_file.exists():\\n        print(f\"{hourly_file.name} already exists. Skipping update.\")\\n        return\\n    entries = collect_headlines()\\n    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\\n    deduped = []\\n    seen_titles = set()\\n    seen_links = set()\\n    for pub, title, link, src in entries:\\n        t_key = title.lower()\\n        l_key = link.lower()\\n        if t_key in seen_titles or l_key in seen_links:\\n            continue\\n        deduped.append((pub, src, title, link))\\n        seen_titles.add(t_key)\\n        seen_links.add(l_key)\\n    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\\n    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\\n    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\\n        writer = csv.writer(f)\\n        writer.writerow([\\'pubdate\\', \\'source\\', \\'title\\', \\'link\\'])\\n        for pub, src, title, link in deduped:\\n            writer.writerow([format_pubdate(pub), src, title, link])\\n    latest_file = HEADLINES_DIR / \"latest.csv\"\\n    shutil.copy(hourly_file, latest_file)\\n    print(f\"Wrote {hourly_file} and updated latest.csv\")\\n    # Get the most recent headline\\n\\n\\nupdate_headlines()\\n\\n\\n\\n\\n\\n', 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'Wrote c:\\\\Users\\\\CJ\\\\Documents\\\\GitHub\\\\Analysis\\\\analysis\\\\headlines\\\\2025-07-07-23-00.csv and updated latest.csv\\n'}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"C:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_22348\\\\2800259878.py:94: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\\n\"}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 3\n",
      "[NbConvertApp] Kernel is taking too long to finish, terminating\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x000002581C3CB020>\n",
      "[NbConvertApp] Writing 8888 bytes to c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\headlines\\.ipynb_checkpoints\\update_headlines-checkpoint.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Running analysis\\headlines\\update_headlines.ipynb …\n",
      "  Command: c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\headlines\\update_headlines.ipynb\n",
      "[NbConvertApp] Searching ['C:\\\\Users\\\\CJ\\\\.jupyter', 'C:\\\\Users\\\\CJ\\\\AppData\\\\Roaming\\\\Python\\\\etc\\\\jupyter', 'c:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\etc\\\\jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in C:\\Users\\CJ\\AppData\\Roaming\\Python\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in C:\\Users\\CJ\\.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in C:\\Users\\CJ\\AppData\\Roaming\\Python\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in C:\\Users\\CJ\\.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\headlines\\update_headlines.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'update_headlines'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['c:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python.exe', '-m', 'ipykernel_launcher', '-f', 'C:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Temp\\\\tmpg2zymbxv.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55720\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:55717\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55717\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:55716\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55716\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:55718\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55718\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:55719\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:55720\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55720\n",
      "C:\\Users\\CJ\\AppData\\Roaming\\Python\\Python312\\site-packages\\zmq\\_future.py:718: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "print('No external dependencies required.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\nprint('No external dependencies required.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'No external dependencies required.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "# Some of the dependencies, to trigger the dependency checker:\n",
      "# data/news-us/nyt/news-us-nyt/latest.json\n",
      "# data/news-us/wsj/news-us-wsj/latest.json\n",
      "\n",
      "\n",
      "from pathlib import Path\n",
      "import csv\n",
      "import json\n",
      "import xml.etree.ElementTree as ET\n",
      "from datetime import datetime, timezone, timedelta\n",
      "from email.utils import parsedate_to_datetime\n",
      "import shutil\n",
      "\n",
      "BASE_DIR = Path.cwd()\n",
      "REPO_DIR = BASE_DIR\n",
      "while not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):\n",
      "    if REPO_DIR.parent == REPO_DIR:\n",
      "        raise FileNotFoundError('Repository root not found')\n",
      "    REPO_DIR = REPO_DIR.parent\n",
      "DATA_DIR = REPO_DIR / 'data'\n",
      "HEADLINES_DIR = REPO_DIR / 'analysis/headlines'\n",
      "HEADLINES_DIR.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "def parse_pubdate(date_str):\n",
      "    try:\n",
      "        dt = parsedate_to_datetime(date_str) if date_str else None\n",
      "        if dt is None:\n",
      "            return None\n",
      "        if dt.tzinfo is None:\n",
      "            dt = dt.replace(tzinfo=timezone.utc)\n",
      "        return dt.astimezone(timezone.utc)\n",
      "    except Exception:\n",
      "        return None\n",
      "\n",
      "def format_pubdate(dt):\n",
      "    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''\n",
      "\n",
      "def parse_feed(path: Path):\n",
      "    entries = []\n",
      "    if path.suffix == '.json':\n",
      "        with open(path, 'r', encoding='utf-8') as f:\n",
      "            data = json.load(f)\n",
      "        for item in data.get('entries', []):\n",
      "            title = item.get('title')\n",
      "            link = item.get('link')\n",
      "            pub = parse_pubdate(item.get('published'))\n",
      "            if title and link:\n",
      "                entries.append((pub, title.strip(), link.strip()))\n",
      "    else:\n",
      "        try:\n",
      "            tree = ET.parse(path)\n",
      "            root = tree.getroot()\n",
      "        except ET.ParseError:\n",
      "            return entries\n",
      "        for item in root.iter():\n",
      "            if item.tag.lower().endswith(('item', 'entry')):\n",
      "                title = None\n",
      "                link = None\n",
      "                pub = None\n",
      "                for child in item:\n",
      "                    tag = child.tag.lower()\n",
      "                    if tag.endswith('title'):\n",
      "                        title = (child.text or '').strip()\n",
      "                    if tag.endswith('link'):\n",
      "                        link = (child.text or '').strip() or child.attrib.get('href')\n",
      "                    if tag.endswith(('pubdate', 'published', 'updated')):\n",
      "                        pub = parse_pubdate((child.text or '').strip())\n",
      "                if title and link:\n",
      "                    entries.append((pub, title, link))\n",
      "    return entries\n",
      "\n",
      "def collect_headlines():\n",
      "    all_entries = []\n",
      "    feed_info = {}\n",
      "    for source in DATA_DIR.iterdir():\n",
      "        if source.is_dir() and source.name.startswith('news'):\n",
      "            candidates = [p for p in source.rglob('latest.*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                candidates = [p for p in source.rglob('*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                continue\n",
      "            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\n",
      "            source_name = latest_file.relative_to(DATA_DIR).parts[1]\n",
      "            feed_entries = parse_feed(latest_file)\n",
      "            if feed_entries:\n",
      "                recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)\n",
      "                feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}\n",
      "            for pub, title, link in feed_entries:\n",
      "                all_entries.append((pub, title, link, source_name))\n",
      "    return all_entries, feed_info\n",
      "\n",
      "def _date_key(date_str):\n",
      "    try:\n",
      "        return parsedate_to_datetime(date_str) if date_str else datetime.min\n",
      "    except Exception:\n",
      "        return datetime.min\n",
      "\n",
      "def update_headlines():\n",
      "    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\n",
      "    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\n",
      "    if hourly_file.exists():\n",
      "        print(f\"{hourly_file.name} already exists. Skipping update.\")\n",
      "        return\n",
      "    entries, feed_info = collect_headlines()\n",
      "    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
      "    deduped = []\n",
      "    seen_titles = set()\n",
      "    seen_links = set()\n",
      "    for pub, title, link, src in entries:\n",
      "        t_key = title.lower()\n",
      "        l_key = link.lower()\n",
      "        if t_key in seen_titles or l_key in seen_links:\n",
      "            continue\n",
      "        deduped.append((pub, src, title, link))\n",
      "        seen_titles.add(t_key)\n",
      "        seen_links.add(l_key)\n",
      "    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\n",
      "    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\n",
      "    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        writer.writerow(['pubdate', 'source', 'title', 'link'])\n",
      "        for pub, src, title, link in deduped:\n",
      "            writer.writerow([format_pubdate(pub), src, title, link])\n",
      "    latest_file = HEADLINES_DIR / \"latest.csv\"\n",
      "    shutil.copy(hourly_file, latest_file)\n",
      "    print(f\"Wrote {hourly_file} and updated latest.csv\")\n",
      "    print()\n",
      "    print('Feed summary:')\n",
      "    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")\n",
      "    for src, info in sorted(feed_info.items()):\n",
      "        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")\n",
      "    # Get the most recent headline\n",
      "\n",
      "\n",
      "update_headlines()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': '# Some of the dependencies, to trigger the dependency checker:\\n# data/news-us/nyt/news-us-nyt/latest.json\\n# data/news-us/wsj/news-us-wsj/latest.json\\n\\n\\nfrom pathlib import Path\\nimport csv\\nimport json\\nimport xml.etree.ElementTree as ET\\nfrom datetime import datetime, timezone, timedelta\\nfrom email.utils import parsedate_to_datetime\\nimport shutil\\n\\nBASE_DIR = Path.cwd()\\nREPO_DIR = BASE_DIR\\nwhile not ((REPO_DIR / \\'data\\').exists() and (REPO_DIR / \\'analysis\\').exists()):\\n    if REPO_DIR.parent == REPO_DIR:\\n        raise FileNotFoundError(\\'Repository root not found\\')\\n    REPO_DIR = REPO_DIR.parent\\nDATA_DIR = REPO_DIR / \\'data\\'\\nHEADLINES_DIR = REPO_DIR / \\'analysis/headlines\\'\\nHEADLINES_DIR.mkdir(parents=True, exist_ok=True)\\n\\ndef parse_pubdate(date_str):\\n    try:\\n        dt = parsedate_to_datetime(date_str) if date_str else None\\n        if dt is None:\\n            return None\\n        if dt.tzinfo is None:\\n            dt = dt.replace(tzinfo=timezone.utc)\\n        return dt.astimezone(timezone.utc)\\n    except Exception:\\n        return None\\n\\ndef format_pubdate(dt):\\n    return dt.strftime(\\'%Y-%m-%d-%H-%M-%S +0000\\') if dt else \\'\\'\\n\\ndef parse_feed(path: Path):\\n    entries = []\\n    if path.suffix == \\'.json\\':\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            data = json.load(f)\\n        for item in data.get(\\'entries\\', []):\\n            title = item.get(\\'title\\')\\n            link = item.get(\\'link\\')\\n            pub = parse_pubdate(item.get(\\'published\\'))\\n            if title and link:\\n                entries.append((pub, title.strip(), link.strip()))\\n    else:\\n        try:\\n            tree = ET.parse(path)\\n            root = tree.getroot()\\n        except ET.ParseError:\\n            return entries\\n        for item in root.iter():\\n            if item.tag.lower().endswith((\\'item\\', \\'entry\\')):\\n                title = None\\n                link = None\\n                pub = None\\n                for child in item:\\n                    tag = child.tag.lower()\\n                    if tag.endswith(\\'title\\'):\\n                        title = (child.text or \\'\\').strip()\\n                    if tag.endswith(\\'link\\'):\\n                        link = (child.text or \\'\\').strip() or child.attrib.get(\\'href\\')\\n                    if tag.endswith((\\'pubdate\\', \\'published\\', \\'updated\\')):\\n                        pub = parse_pubdate((child.text or \\'\\').strip())\\n                if title and link:\\n                    entries.append((pub, title, link))\\n    return entries\\n\\ndef collect_headlines():\\n    all_entries = []\\n    feed_info = {}\\n    for source in DATA_DIR.iterdir():\\n        if source.is_dir() and source.name.startswith(\\'news\\'):\\n            candidates = [p for p in source.rglob(\\'latest.*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                candidates = [p for p in source.rglob(\\'*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                continue\\n            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\\n            source_name = latest_file.relative_to(DATA_DIR).parts[1]\\n            feed_entries = parse_feed(latest_file)\\n            if feed_entries:\\n                recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)\\n                feed_info[source_name] = {\\'count\\': len(feed_entries), \\'recent\\': recent}\\n            for pub, title, link in feed_entries:\\n                all_entries.append((pub, title, link, source_name))\\n    return all_entries, feed_info\\n\\ndef _date_key(date_str):\\n    try:\\n        return parsedate_to_datetime(date_str) if date_str else datetime.min\\n    except Exception:\\n        return datetime.min\\n\\ndef update_headlines():\\n    timestamp = datetime.utcnow().strftime(\\'%Y-%m-%d-%H-00\\')\\n    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\\n    if hourly_file.exists():\\n        print(f\"{hourly_file.name} already exists. Skipping update.\")\\n        return\\n    entries, feed_info = collect_headlines()\\n    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\\n    deduped = []\\n    seen_titles = set()\\n    seen_links = set()\\n    for pub, title, link, src in entries:\\n        t_key = title.lower()\\n        l_key = link.lower()\\n        if t_key in seen_titles or l_key in seen_links:\\n            continue\\n        deduped.append((pub, src, title, link))\\n        seen_titles.add(t_key)\\n        seen_links.add(l_key)\\n    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\\n    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\\n    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\\n        writer = csv.writer(f)\\n        writer.writerow([\\'pubdate\\', \\'source\\', \\'title\\', \\'link\\'])\\n        for pub, src, title, link in deduped:\\n            writer.writerow([format_pubdate(pub), src, title, link])\\n    latest_file = HEADLINES_DIR / \"latest.csv\"\\n    shutil.copy(hourly_file, latest_file)\\n    print(f\"Wrote {hourly_file} and updated latest.csv\")\\n    print()\\n    print(\\'Feed summary:\\')\\n    print(f\"{\\'source\\':<20} {\\'count\\':>5}  {\\'most recent\\'}\")\\n    for src, info in sorted(feed_info.items()):\\n        print(f\"{src:<20} {info[\\'count\\']:5}  {format_pubdate(info[\\'recent\\'])}\")\\n    # Get the most recent headline\\n\\n\\nupdate_headlines()\\n\\n\\n\\n\\n\\n', 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': '2025-07-07-23-00.csv already exists. Skipping update.\\n'}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"C:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_13660\\\\1894882967.py:99: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\\n\"}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Kernel is taking too long to finish, terminating\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x000001EF04AD08F0>\n",
      "[NbConvertApp] Writing 9243 bytes to c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\headlines\\update_headlines.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Running analysis\\news-topics\\analyze_headlines.ipynb …\n",
      "  Command: c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\news-topics\\analyze_headlines.ipynb\n",
      "[NbConvertApp] Searching ['C:\\\\Users\\\\CJ\\\\.jupyter', 'C:\\\\Users\\\\CJ\\\\AppData\\\\Roaming\\\\Python\\\\etc\\\\jupyter', 'c:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\etc\\\\jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in C:\\Users\\CJ\\AppData\\Roaming\\Python\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in C:\\Users\\CJ\\.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in c:\\Users\\CJ\\AppData\\Local\\Programs\\Python\\Python312\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in C:\\Users\\CJ\\AppData\\Roaming\\Python\\etc\\jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in C:\\Users\\CJ\\.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\news-topics\\analyze_headlines.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'analyze_headlines'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['c:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python.exe', '-m', 'ipykernel_launcher', '-f', 'C:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Temp\\\\tmpxcjp3cih.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55829\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:55826\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55826\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:55825\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55825\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:55827\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55827\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:55828\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:55829\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:55829\n",
      "C:\\Users\\CJ\\AppData\\Roaming\\Python\\Python312\\site-packages\\zmq\\_future.py:718: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "_ensure('pandas')\n",
      "print('All dependencies ready.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\n_ensure('pandas')\\nprint('All dependencies ready.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'All dependencies ready.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "import pandas as pd\n",
      "latest = pd.read_csv('../headlines/latest.csv')\n",
      "latest.head()\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import pandas as pd\\nlatest = pd.read_csv('../headlines/latest.csv')\\nlatest.head()\", 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': '                     pubdate source  \\\\\\n0  2025-07-07-23-16-00 +0000    wsj   \\n1  2025-07-07-22-36-38 +0000    nyt   \\n2  2025-07-07-22-22-00 +0000    bbc   \\n3  2025-07-07-22-01-54 +0000    nyt   \\n4  2025-07-07-21-51-39 +0000    nyt   \\n\\n                                               title  \\\\\\n0  President Trump reignited his global trade war...   \\n1  Haitiâ€™s Landmark Oloffson Hotel is Destroyed i...   \\n2  Young campers, teachers and football coach amo...   \\n3  As Renewed U.S. Tariffs Loom, Emerging Economi...   \\n4  New Document Undermines Trump Administrationâ€™s...   \\n\\n                                                link  \\n0  https://www.wsj.com/economy/trade/trump-tariff...  \\n1  https://www.nytimes.com/2025/07/07/world/ameri...  \\n2     https://www.bbc.com/news/articles/c5ygl8lpyyqo  \\n3  https://www.nytimes.com/2025/07/07/world/ameri...  \\n4  https://www.nytimes.com/2025/07/07/us/politics...  ', 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>2025-07-07-23-16-00 +0000</td>\\n      <td>wsj</td>\\n      <td>President Trump reignited his global trade war...</td>\\n      <td>https://www.wsj.com/economy/trade/trump-tariff...</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2025-07-07-22-36-38 +0000</td>\\n      <td>nyt</td>\\n      <td>Haitiâ€™s Landmark Oloffson Hotel is Destroyed i...</td>\\n      <td>https://www.nytimes.com/2025/07/07/world/ameri...</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2025-07-07-22-22-00 +0000</td>\\n      <td>bbc</td>\\n      <td>Young campers, teachers and football coach amo...</td>\\n      <td>https://www.bbc.com/news/articles/c5ygl8lpyyqo</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>2025-07-07-22-01-54 +0000</td>\\n      <td>nyt</td>\\n      <td>As Renewed U.S. Tariffs Loom, Emerging Economi...</td>\\n      <td>https://www.nytimes.com/2025/07/07/world/ameri...</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>2025-07-07-21-51-39 +0000</td>\\n      <td>nyt</td>\\n      <td>New Document Undermines Trump Administrationâ€™s...</td>\\n      <td>https://www.nytimes.com/2025/07/07/us/politics...</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 3\n",
      "[NbConvertApp] Executing cell:\n",
      "import re\n",
      "from collections import Counter\n",
      "from datetime import datetime\n",
      "\n",
      "with open('exclude.txt') as f:\n",
      "    stop_words = set(w.strip() for w in f if w.strip())\n",
      "words = re.findall(r'[A-Za-z]+', ' '.join(latest['title']).lower())\n",
      "filtered = [w for w in words if w not in stop_words and len(w) > 1]\n",
      "counts = Counter(filtered)\n",
      "score_df = (\n",
      "    pd.DataFrame(counts.items(), columns=['word','score'])\n",
      "    .sort_values('score', ascending=False)\n",
      ")\n",
      "score_df[['score','word']].to_csv('scores.csv', index=False)\n",
      "timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\n",
      "score_df[['score','word']].to_csv(f'scores-{timestamp}.csv', index=False)\n",
      "score_df.head()\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import re\\nfrom collections import Counter\\nfrom datetime import datetime\\n\\nwith open('exclude.txt') as f:\\n    stop_words = set(w.strip() for w in f if w.strip())\\nwords = re.findall(r'[A-Za-z]+', ' '.join(latest['title']).lower())\\nfiltered = [w for w in words if w not in stop_words and len(w) > 1]\\ncounts = Counter(filtered)\\nscore_df = (\\n    pd.DataFrame(counts.items(), columns=['word','score'])\\n    .sort_values('score', ascending=False)\\n)\\nscore_df[['score','word']].to_csv('scores.csv', index=False)\\ntimestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\\nscore_df[['score','word']].to_csv(f'scores-{timestamp}.csv', index=False)\\nscore_df.head()\\n\", 'execution_count': 3}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"C:\\\\Users\\\\CJ\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_5232\\\\2741963311.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\\n\"}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': '         word  score\\n1       trump     10\\n34      texas      6\\n4       trade      4\\n12    tariffs      4\\n64  countries      3', 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>word</th>\\n      <th>score</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>1</th>\\n      <td>trump</td>\\n      <td>10</td>\\n    </tr>\\n    <tr>\\n      <th>34</th>\\n      <td>texas</td>\\n      <td>6</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>trade</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>12</th>\\n      <td>tariffs</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>64</th>\\n      <td>countries</td>\\n      <td>3</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 3}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 5\n",
      "[NbConvertApp] Executing cell:\n",
      "word_scores = dict(score_df[['word','score']].values)\n",
      "latest['score'] = latest['title'].apply(\n",
      "    lambda t: sum(\n",
      "        word_scores.get(w.lower(), 0)\n",
      "        for w in re.findall(r'[A-Za-z]+', t)\n",
      "        if len(w) > 1\n",
      "    )\n",
      ")\n",
      "ranked = latest.sort_values('score', ascending=False)\n",
      "ranked[['score','pubdate','source','title','link']].to_csv('rank.csv', index=False)\n",
      "ranked[['score','pubdate','source','title','link']].to_csv(f'rank-{timestamp}.csv', index=False)\n",
      "ranked.head()\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"word_scores = dict(score_df[['word','score']].values)\\nlatest['score'] = latest['title'].apply(\\n    lambda t: sum(\\n        word_scores.get(w.lower(), 0)\\n        for w in re.findall(r'[A-Za-z]+', t)\\n        if len(w) > 1\\n    )\\n)\\nranked = latest.sort_values('score', ascending=False)\\nranked[['score','pubdate','source','title','link']].to_csv('rank.csv', index=False)\\nranked[['score','pubdate','source','title','link']].to_csv(f'rank-{timestamp}.csv', index=False)\\nranked.head()\\n\", 'execution_count': 4}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': '                      pubdate source  \\\\\\n35  2025-07-07-10-57-00 +0000    wsj   \\n0   2025-07-07-23-16-00 +0000    wsj   \\n12  2025-07-07-21-08-00 +0000    wsj   \\n39  2025-07-07-05-17-39 +0000    bbc   \\n7   2025-07-07-21-43-18 +0000    bbc   \\n\\n                                                title  \\\\\\n35  President Trump is facing a pivotal time for r...   \\n0   President Trump reignited his global trade war...   \\n12  Hereâ€™s an annotated look at Trumpâ€™s tariff let...   \\n39  Netanyahu visits US as Trump puts pressure to ...   \\n7   Trump threatens tariffs on 14 countries from A...   \\n\\n                                                 link  score  \\n35  https://www.wsj.com/economy/trade/trump-faces-...     42  \\n0   https://www.wsj.com/economy/trade/trump-tariff...     39  \\n12  https://www.wsj.com/politics/policy/read-an-an...     29  \\n39     https://www.bbc.com/news/articles/cy4ypze027ro     21  \\n7      https://www.bbc.com/news/articles/cd0vkl31085o     20  ', 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n      <th>score</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>35</th>\\n      <td>2025-07-07-10-57-00 +0000</td>\\n      <td>wsj</td>\\n      <td>President Trump is facing a pivotal time for r...</td>\\n      <td>https://www.wsj.com/economy/trade/trump-faces-...</td>\\n      <td>42</td>\\n    </tr>\\n    <tr>\\n      <th>0</th>\\n      <td>2025-07-07-23-16-00 +0000</td>\\n      <td>wsj</td>\\n      <td>President Trump reignited his global trade war...</td>\\n      <td>https://www.wsj.com/economy/trade/trump-tariff...</td>\\n      <td>39</td>\\n    </tr>\\n    <tr>\\n      <th>12</th>\\n      <td>2025-07-07-21-08-00 +0000</td>\\n      <td>wsj</td>\\n      <td>Hereâ€™s an annotated look at Trumpâ€™s tariff let...</td>\\n      <td>https://www.wsj.com/politics/policy/read-an-an...</td>\\n      <td>29</td>\\n    </tr>\\n    <tr>\\n      <th>39</th>\\n      <td>2025-07-07-05-17-39 +0000</td>\\n      <td>bbc</td>\\n      <td>Netanyahu visits US as Trump puts pressure to ...</td>\\n      <td>https://www.bbc.com/news/articles/cy4ypze027ro</td>\\n      <td>21</td>\\n    </tr>\\n    <tr>\\n      <th>7</th>\\n      <td>2025-07-07-21-43-18 +0000</td>\\n      <td>bbc</td>\\n      <td>Trump threatens tariffs on 14 countries from A...</td>\\n      <td>https://www.bbc.com/news/articles/cd0vkl31085o</td>\\n      <td>20</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 4}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 7\n",
      "[NbConvertApp] Executing cell:\n",
      "top_rows = []\n",
      "working = word_scores.copy()\n",
      "remaining = latest.copy()\n",
      "for _ in range(10):\n",
      "    ranked_loop = remaining.assign(score=remaining['title'].apply(\n",
      "        lambda t: sum(working.get(w.lower(), 0)\n",
      "                      for w in re.findall(r'[A-Za-z]+', t)\n",
      "                      if len(w) > 1)\n",
      "    )).sort_values('score', ascending=False)\n",
      "    if ranked_loop.empty:\n",
      "        break\n",
      "    top_story = ranked_loop.iloc[0]\n",
      "    top_rows.append(top_story[['score','pubdate','source','title','link']])\n",
      "    words = set(re.findall(r'[A-Za-z]+', top_story['title'].lower()))\n",
      "    for w in words:\n",
      "        working.pop(w, None)\n",
      "    remaining = remaining.drop(top_story.name)\n",
      "top_df = pd.DataFrame(top_rows)\n",
      "top_df.to_csv('top.csv', index=False)\n",
      "top_df.to_csv(f'top-{timestamp}.csv', index=False)\n",
      "top_df\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"top_rows = []\\nworking = word_scores.copy()\\nremaining = latest.copy()\\nfor _ in range(10):\\n    ranked_loop = remaining.assign(score=remaining['title'].apply(\\n        lambda t: sum(working.get(w.lower(), 0)\\n                      for w in re.findall(r'[A-Za-z]+', t)\\n                      if len(w) > 1)\\n    )).sort_values('score', ascending=False)\\n    if ranked_loop.empty:\\n        break\\n    top_story = ranked_loop.iloc[0]\\n    top_rows.append(top_story[['score','pubdate','source','title','link']])\\n    words = set(re.findall(r'[A-Za-z]+', top_story['title'].lower()))\\n    for w in words:\\n        working.pop(w, None)\\n    remaining = remaining.drop(top_story.name)\\ntop_df = pd.DataFrame(top_rows)\\ntop_df.to_csv('top.csv', index=False)\\ntop_df.to_csv(f'top-{timestamp}.csv', index=False)\\ntop_df\\n\", 'execution_count': 5}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': \"    score                    pubdate source  \\\\\\n35     42  2025-07-07-10-57-00 +0000    wsj   \\n2      17  2025-07-07-22-22-00 +0000    bbc   \\n12     16  2025-07-07-21-08-00 +0000    wsj   \\n0      14  2025-07-07-23-16-00 +0000    wsj   \\n32     12  2025-07-07-13-06-32 +0000    bbc   \\n39     11  2025-07-07-05-17-39 +0000    bbc   \\n1       9  2025-07-07-22-36-38 +0000    nyt   \\n20      9  2025-07-07-17-42-19 +0000    bbc   \\n16      9  2025-07-07-19-36-43 +0000    bbc   \\n40      9  2025-07-07-02-48-36 +0000    bbc   \\n\\n                                                title  \\\\\\n35  President Trump is facing a pivotal time for r...   \\n2   Young campers, teachers and football coach amo...   \\n12  Hereâ€™s an annotated look at Trumpâ€™s tariff let...   \\n0   President Trump reignited his global trade war...   \\n32  South Africa's police minister accused of link...   \\n39  Netanyahu visits US as Trump puts pressure to ...   \\n1   Haitiâ€™s Landmark Oloffson Hotel is Destroyed i...   \\n20  US to remove Syria's HTS from list of foreign ...   \\n16  Former Tory cabinet minister Jones joins Refor...   \\n40  Government urged to keep education plans for c...   \\n\\n                                                 link  \\n35  https://www.wsj.com/economy/trade/trump-faces-...  \\n2      https://www.bbc.com/news/articles/c5ygl8lpyyqo  \\n12  https://www.wsj.com/politics/policy/read-an-an...  \\n0   https://www.wsj.com/economy/trade/trump-tariff...  \\n32     https://www.bbc.com/news/articles/cx205ykppdjo  \\n39     https://www.bbc.com/news/articles/cy4ypze027ro  \\n1   https://www.nytimes.com/2025/07/07/world/ameri...  \\n20     https://www.bbc.com/news/articles/cj0m194v974o  \\n16     https://www.bbc.com/news/articles/c93kwk00gkgo  \\n40     https://www.bbc.com/news/articles/cx2vn950d5go  \", 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>score</th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>35</th>\\n      <td>42</td>\\n      <td>2025-07-07-10-57-00 +0000</td>\\n      <td>wsj</td>\\n      <td>President Trump is facing a pivotal time for r...</td>\\n      <td>https://www.wsj.com/economy/trade/trump-faces-...</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>17</td>\\n      <td>2025-07-07-22-22-00 +0000</td>\\n      <td>bbc</td>\\n      <td>Young campers, teachers and football coach amo...</td>\\n      <td>https://www.bbc.com/news/articles/c5ygl8lpyyqo</td>\\n    </tr>\\n    <tr>\\n      <th>12</th>\\n      <td>16</td>\\n      <td>2025-07-07-21-08-00 +0000</td>\\n      <td>wsj</td>\\n      <td>Hereâ€™s an annotated look at Trumpâ€™s tariff let...</td>\\n      <td>https://www.wsj.com/politics/policy/read-an-an...</td>\\n    </tr>\\n    <tr>\\n      <th>0</th>\\n      <td>14</td>\\n      <td>2025-07-07-23-16-00 +0000</td>\\n      <td>wsj</td>\\n      <td>President Trump reignited his global trade war...</td>\\n      <td>https://www.wsj.com/economy/trade/trump-tariff...</td>\\n    </tr>\\n    <tr>\\n      <th>32</th>\\n      <td>12</td>\\n      <td>2025-07-07-13-06-32 +0000</td>\\n      <td>bbc</td>\\n      <td>South Africa\\'s police minister accused of link...</td>\\n      <td>https://www.bbc.com/news/articles/cx205ykppdjo</td>\\n    </tr>\\n    <tr>\\n      <th>39</th>\\n      <td>11</td>\\n      <td>2025-07-07-05-17-39 +0000</td>\\n      <td>bbc</td>\\n      <td>Netanyahu visits US as Trump puts pressure to ...</td>\\n      <td>https://www.bbc.com/news/articles/cy4ypze027ro</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>9</td>\\n      <td>2025-07-07-22-36-38 +0000</td>\\n      <td>nyt</td>\\n      <td>Haitiâ€™s Landmark Oloffson Hotel is Destroyed i...</td>\\n      <td>https://www.nytimes.com/2025/07/07/world/ameri...</td>\\n    </tr>\\n    <tr>\\n      <th>20</th>\\n      <td>9</td>\\n      <td>2025-07-07-17-42-19 +0000</td>\\n      <td>bbc</td>\\n      <td>US to remove Syria\\'s HTS from list of foreign ...</td>\\n      <td>https://www.bbc.com/news/articles/cj0m194v974o</td>\\n    </tr>\\n    <tr>\\n      <th>16</th>\\n      <td>9</td>\\n      <td>2025-07-07-19-36-43 +0000</td>\\n      <td>bbc</td>\\n      <td>Former Tory cabinet minister Jones joins Refor...</td>\\n      <td>https://www.bbc.com/news/articles/c93kwk00gkgo</td>\\n    </tr>\\n    <tr>\\n      <th>40</th>\\n      <td>9</td>\\n      <td>2025-07-07-02-48-36 +0000</td>\\n      <td>bbc</td>\\n      <td>Government urged to keep education plans for c...</td>\\n      <td>https://www.bbc.com/news/articles/cx2vn950d5go</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 5}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 9\n",
      "[NbConvertApp] Executing cell:\n",
      "import pandas as pd\n",
      "pd.read_csv('top.csv').to_json('top.json', orient='records', indent=2)\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import pandas as pd\\npd.read_csv('top.csv').to_json('top.json', orient='records', indent=2)\\n\", 'execution_count': 6}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 11\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x0000015DA3F2AA80>\n",
      "[NbConvertApp] Writing 23578 bytes to c:\\Users\\CJ\\Documents\\GitHub\\Analysis\\analysis\\news-topics\\analyze_headlines.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Everything up to date.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update outdated notebooks until none remain\n",
    "import json, re, time, subprocess, sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_dir = Path.cwd()\n",
    "if not (repo_dir / 'analysis').is_dir():\n",
    "    repo_dir = repo_dir.parent\n",
    "analysis_dir = repo_dir / 'analysis'\n",
    "data_dir = repo_dir / 'data'\n",
    "\n",
    "pattern = re.compile(r'[A-Za-z0-9_/.-]*latest\\.(?:csv|json|xml|rss)')\n",
    "\n",
    "def mtime_str(p: Path) -> str:\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(p.stat().st_mtime))\n",
    "\n",
    "def build_dep_map():\n",
    "    ipynb_paths = sorted(analysis_dir.rglob('*.ipynb'))\n",
    "    dep_map = {}\n",
    "    for nb in ipynb_paths:\n",
    "        text = nb.read_text()\n",
    "        matches = sorted(set(pattern.findall(text)))\n",
    "        deps = []\n",
    "        for m in matches:\n",
    "            dep = (nb.parent / m).resolve()\n",
    "            if not dep.exists():\n",
    "                dep = (repo_dir / m.lstrip('./')).resolve()\n",
    "            if dep.exists():\n",
    "                deps.append(dep)\n",
    "        dep_map[nb] = deps\n",
    "    return dep_map\n",
    "\n",
    "def outdated(nb, deps):\n",
    "    nb_mtime = nb.stat().st_mtime\n",
    "    return any(d.stat().st_mtime > nb_mtime for d in deps)\n",
    "\n",
    "\n",
    "def execute(nb: Path):\n",
    "    import shutil\n",
    "    rel = str(nb.relative_to(repo_dir))\n",
    "    if not shutil.which('jupyter'):\n",
    "        print(f'jupyter not available - skipping {rel}')\n",
    "        return\n",
    "    print(f'Running {rel} …')\n",
    "    cmd=[sys.executable,'-m','jupyter','nbconvert','--to','notebook','--inplace','--execute','--ExecutePreprocessor.timeout=600','--debug',str(nb)]\n",
    "    print('  Command:', ' '.join(cmd))\n",
    "    proc=subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    print(proc.stdout)\n",
    "    if proc.returncode==0:\n",
    "        print('  ✓ success')\n",
    "    else:\n",
    "        print(f'  ✗ failed with exit code {proc.returncode}')\n",
    "while True:\n",
    "    dep_map=build_dep_map()\n",
    "    outdated_nbs=[nb for nb,deps in dep_map.items() if deps and outdated(nb,deps)]\n",
    "    report={'outdated_notebooks':[str(nb.relative_to(repo_dir)) for nb in outdated_nbs]}\n",
    "    deps_path = repo_dir / 'dependencies.json'\n",
    "    deps_path.write_text(\n",
    "        json.dumps(report, indent=2) + \"\\n\"\n",
    "    )\n",
    "    if not outdated_nbs:\n",
    "        print('Everything up to date.')\n",
    "        break\n",
    "    for nb in outdated_nbs:\n",
    "        execute(nb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
