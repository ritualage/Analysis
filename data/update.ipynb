{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c898f38d",
   "metadata": {},
   "source": [
    "# Update Script\n",
    "This notebook orchestrates data downloads and analysis refreshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d400efd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T05:51:59.058475Z",
     "iopub.status.busy": "2025-07-12T05:51:59.058281Z",
     "iopub.status.idle": "2025-07-12T05:53:25.371806Z",
     "shell.execute_reply": "2025-07-12T05:53:25.371291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'pandas' not found - installing ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/16.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m232.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [tzdata]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [numpy]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [pandas]\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.3.1 pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'feedparser' not found - installing ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=61179b7c8d53f348f070a17b7a71ec8caf31afc3b115a569c4f63e1437bb471b\n",
      "  Stored in directory: /home/runner/.cache/pip/wheels/3d/4d/ef/37cdccc18d6fd7e0dd7817dcdf9146d4d6789c32a227a28134\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [feedparser]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'textblob' not found - installing ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk>=3.9->textblob)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk>=3.9->textblob)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex>=2021.8.3 (from nltk>=3.9->textblob)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk>=3.9->textblob)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/624.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/796.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tqdm, regex, joblib, click, nltk, textblob\n",
      "\u001b[?25l\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [joblib]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [joblib]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nltk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m5/6\u001b[0m [textblob]\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [textblob]\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 textblob-0.19.0 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies ready.\n",
      "\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching GDPC1 … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=GDPC1&file_type=json&observation_end=2025-07-12\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching A939RX0Q048SBEA … ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=A939RX0Q048SBEA&file_type=json&observation_end=2025-07-12\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching M2REAL … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=M2REAL&file_type=json&observation_end=2025-07-12\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching UNRATE … ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=UNRATE&file_type=json&observation_end=2025-07-12\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching CLVMNACSCAB1GQDE … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=CLVMNACSCAB1GQDE&file_type=json&observation_end=2025-07-12\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching GFDEBTN … ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=GFDEBTN&file_type=json&observation_end=2025-07-12\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching GFDEGDQ188S … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=GFDEGDQ188S&file_type=json&observation_end=2025-07-12\n",
      "Warning: environment variable 'FRED_API_KEY' not set.\n",
      "Fetching TDSP … ✗ failed: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series/observations?series_id=TDSP&file_type=json&observation_end=2025-07-12\n",
      "Fetching news-us-nyt … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-world-nyt … ✓ success\n",
      "Fetching news-africa-nyt … no change\n",
      "Fetching news-europe-nyt … ✓ success\n",
      "Fetching news-asia-nyt … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-americas-nyt … no change\n",
      "Fetching news-middle-east-nyt … ✓ success\n",
      "Fetching news-business-nyt … ✓ success\n",
      "Fetching news-economy-nyt … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-us-politics-nyt … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-world-wsj … no change\n",
      "Fetching news-us-wsj … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-business-wsj … ✓ success\n",
      "Fetching news-markets-wsj … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-economy-wsj … no change\n",
      "Fetching news-us-politics-wsj … no change\n",
      "Fetching news-us-politics-wapo … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ failed: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=30)\n",
      "Fetching news-us-wapo … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-world-wapo … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-business-wapo … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching latimes-business … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching latimes-us … ✓ success\n",
      "Fetching latimes-us-politics … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-world-chi-tribune … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-business-chi-tribune … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-us-politics-chi-tribune … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-us-business-startribune … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-us-politics-startribune … ✗ failed: 404 Client Error: Not Found for url: https://www.startribune.com/politics/index.rss2\n",
      "Fetching news-us-nypost … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-world-nypost … no change\n",
      "Fetching news-us-politics-nypost … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-business-nypost … no change\n",
      "Fetching news-world-toi … ✓ success\n",
      "Fetching news-business-toi … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching news-us-toi … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-middle-east-toi … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-europe-toi … no change\n",
      "Fetching news-world-cbc … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-politics-cbc … no change\n",
      "Fetching news-africa-bbc … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-asia-bbc … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-europe-bbc … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-latin-america-bbc … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-middle-east-bbc … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-us-bbc … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-world-bbc … ✓ success\n",
      "Fetching news-business-bbc … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-politics-bbc … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-top-dw … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-europe-dw … no change\n",
      "Fetching news-world-dw … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "Fetching news-business-dw … no change\n",
      "Fetching news-asia-dw … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ success\n",
      "Fetching zip-demo-ca … ✗ failed: Invalid URL 'nan': No scheme supplied. Perhaps you meant https://nan?\n",
      "\n",
      "Updated: news-us-nyt, news-world-nyt, news-europe-nyt, news-asia-nyt, news-middle-east-nyt, news-business-nyt, news-us-politics-nyt, news-us-wsj, news-business-wsj, news-markets-wsj, news-world-wapo, latimes-business, latimes-us, latimes-us-politics, news-us-business-startribune, news-world-toi, news-business-toi, news-world-bbc, news-asia-dw\n"
     ]
    }
   ],
   "source": [
    "# ========== Bootstrap: ensure required Python packages are present ==========\n",
    "import importlib, subprocess, sys\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def _ensure(pkg_name: str, import_name: Optional[str] = None, required: bool = True):\n",
    "    \"\"\"Import a module, installing it if necessary. If installation fails and\n",
    "    the package is required, the exception is raised. Optional packages may\n",
    "    remain unavailable.\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(import_name or pkg_name)\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"Package '{pkg_name}' not found - installing ...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to install {pkg_name}: {e}\")\n",
    "            if required:\n",
    "                raise\n",
    "    try:\n",
    "        mod = importlib.import_module(import_name or pkg_name)\n",
    "        globals()[import_name or pkg_name] = mod\n",
    "        return mod\n",
    "    except ModuleNotFoundError:\n",
    "        if required:\n",
    "            raise\n",
    "        print(f\"Package '{pkg_name}' is unavailable.\")\n",
    "        globals()[import_name or pkg_name] = None\n",
    "        return None\n",
    "# --- Required third-party libraries ------------------------------------------\n",
    "_ensure(\"pandas\")\n",
    "_ensure(\"requests\")\n",
    "_ensure(\"feedparser\")\n",
    "_ensure(\"textblob\")\n",
    "_ensure(\"jupyter\", required=False)\n",
    "_ensure(\"nbconvert\", required=False)\n",
    "print(\"All dependencies ready.\\n\")\n",
    "\n",
    "# --- Standard imports --------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import os, re, shutil, json, feedparser, textblob\n",
    "import pandas as pd, requests, urllib.parse\n",
    "\n",
    "# --- Helper: replace [date %Y-%m-%d] tokens -----------------------------------\n",
    "def substitute_date_tokens(url: str) -> str:\n",
    "    def _replace(m):\n",
    "        fmt = m.group(1).strip()\n",
    "        return dt.date.today().strftime(fmt)\n",
    "    return re.sub(r\"\\[date\\s+([^\\]]+)\\]\", _replace, url)\n",
    "\n",
    "# --- Helper: append API key if specified -----------------------------------\n",
    "def add_apikey(url: str, env_var: Optional[str]) -> str:\n",
    "    if env_var and str(env_var).lower() != \"nan\":\n",
    "        key = os.getenv(env_var)\n",
    "        if key:\n",
    "            sep = '&' if '?' in url else '?'\n",
    "            return f'{url}{sep}api_key={urllib.parse.quote_plus(key)}'\n",
    "        else:\n",
    "            print(f\"Warning: environment variable '{env_var}' not set.\")\n",
    "    return url\n",
    "\n",
    "# --- Cadence map (word → minimum seconds between fetches) ------------------------\n",
    "CADENCE_SECONDS = {\n",
    "    \"hourly\": 3600,\n",
    "    \"daily\": 86400,\n",
    "    \"weekly\": 604800,\n",
    "    \"monthly\": 2592000,\n",
    "    \"quarterly\": 7776000,\n",
    "}\n",
    "\n",
    "# --- Resolve base directory so notebook works from repo root or data folder ---\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "\n",
    "# --- Load catalog -------------------------------------------------------------\n",
    "catalog_path = BASE_DIR / 'catalog.csv'\n",
    "cat = pd.read_csv(catalog_path)\n",
    "cat['filetype'] = cat['filetype'].astype(str).str.strip().str.lstrip('.')\n",
    "\n",
    "now = dt.datetime.now()\n",
    "today = now.date()\n",
    "updated_rows = []                # remember which rows we refresh\n",
    "\n",
    "for idx, row in cat.iterrows():\n",
    "    folder = BASE_DIR / str(row['category']) / str(row['source']) / str(row['folder'])\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    cadence = str(row['cadence']).lower().strip()\n",
    "    filetype = str(row['filetype']).strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    latest_fp = folder / f'latest.{output_ext}'\n",
    "    url = str(row.get('url', '')).strip()\n",
    "    if not url or url.lower() in ('n/a', 'na', 'none'):\n",
    "        print(f\"Skipping {row['folder']} (static)\")\n",
    "        continue\n",
    "    dated_fp = folder / f\"{now:%Y-%m-%d-%H}.{output_ext}\" if cadence == \"hourly\" else folder / f\"{today:%Y-%m-%d}.{output_ext}\"\n",
    "    if dated_fp.exists():\n",
    "        if (not latest_fp.exists()) or latest_fp.read_bytes() != dated_fp.read_bytes():\n",
    "            shutil.copyfile(dated_fp, latest_fp)\n",
    "        cat.at[idx, 'last_fetched'] = now.isoformat(timespec='minutes')\n",
    "        continue\n",
    "    last_fetched = (\n",
    "        pd.to_datetime(row[\"last_fetched\"])\n",
    "        if pd.notna(row[\"last_fetched\"]) else None\n",
    "    )\n",
    "\n",
    "    # ---- Determine if an update is due --------------------------------------\n",
    "    cadence = str(row[\"cadence\"]).lower().strip()\n",
    "    min_age = CADENCE_SECONDS.get(cadence, 30*86400)        # default 30 days\n",
    "    needs_update = (\n",
    "        (not latest_fp.exists()) or\n",
    "        (not last_fetched) or\n",
    "        (now - last_fetched).total_seconds() >= min_age\n",
    "    )\n",
    "\n",
    "    #if not needs_update:\n",
    "        #print(f\"Skipping {row['folder']} - up to date\")\n",
    "        #continue\n",
    "\n",
    "    # ---- Build the request URL ---------------------------------------------\n",
    "    url = substitute_date_tokens(str(row[\"url\"]))\n",
    "    url = add_apikey(url, str(row.get('api_key') or '').strip() or None)\n",
    "\n",
    "    print(f\"Fetching {row['folder']} …\", end=\" \")\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        r.raise_for_status()\n",
    "        if filetype.lower() in ('rss', 'xml'):\n",
    "            feed = feedparser.parse(r.content)\n",
    "            entries = []\n",
    "            for e in feed.entries:\n",
    "                text = ' '.join(filter(None, [e.get('title'), e.get('summary')]))\n",
    "                polarity = textblob.TextBlob(text).sentiment.polarity\n",
    "                entries.append({'title': e.get('title'), 'link': e.get('link'),\n",
    "                               'published': e.get('published'),\n",
    "                               'sentiment': polarity})\n",
    "            content_bytes = json.dumps({'entries': entries}, ensure_ascii=False, indent=2).encode('utf-8')\n",
    "        else:\n",
    "            content_bytes = r.content\n",
    "        if filetype.lower() == 'json':\n",
    "            try:\n",
    "                data_json = r.json()\n",
    "            except Exception:\n",
    "                data_json = None\n",
    "            if isinstance(data_json, dict) and data_json.get('error_message'):\n",
    "                raise ValueError(data_json['error_message'])\n",
    "        # ---- Save snapshot and latest --------------------------------------\n",
    "        if latest_fp.exists() and latest_fp.read_bytes() == content_bytes:\n",
    "            cat.at[idx, 'last_fetched'] = now.isoformat(timespec='minutes')\n",
    "            print('no change')\n",
    "            continue\n",
    "        dated_fp.write_bytes(content_bytes)\n",
    "        shutil.copyfile(dated_fp, latest_fp)\n",
    "\n",
    "        # ---- Mark success in catalog ---------------------------------------\n",
    "        cat.at[idx, \"last_fetched\"] = now.isoformat(timespec='minutes')\n",
    "        updated_rows.append(row[\"folder\"])\n",
    "        print(\"✓ success\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ failed: {e}\")\n",
    "\n",
    "# --- Persist catalog if anything changed -------------------------------------\n",
    "if updated_rows:\n",
    "    cat.to_csv(catalog_path, index=False)\n",
    "    print(\"\\nUpdated:\", \", \".join(updated_rows))\n",
    "else:\n",
    "    print(\"Everything up to date.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c044a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T05:53:25.373853Z",
     "iopub.status.busy": "2025-07-12T05:53:25.373491Z",
     "iopub.status.idle": "2025-07-12T05:53:25.411618Z",
     "shell.execute_reply": "2025-07-12T05:53:25.411145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index files generated for GDPC1, A939RX0Q048SBEA, M2REAL, UNRATE, CLVMNACSCAB1GQDE, GFDEBTN, GFDEGDQ188S, TDSP, news-us-nyt, news-world-nyt, news-africa-nyt, news-europe-nyt, news-asia-nyt, news-americas-nyt, news-middle-east-nyt, news-business-nyt, news-economy-nyt, news-us-politics-nyt, news-world-wsj, news-us-wsj, news-business-wsj, news-markets-wsj, news-economy-wsj, news-us-politics-wsj, news-us-politics-wapo, news-us-wapo, news-world-wapo, news-business-wapo, latimes-business, latimes-us, latimes-us-politics, news-world-chi-tribune, news-business-chi-tribune, news-us-politics-chi-tribune, news-us-business-startribune, news-us-politics-startribune, news-us-nypost, news-world-nypost, news-us-politics-nypost, news-business-nypost, news-world-toi, news-business-toi, news-us-toi, news-middle-east-toi, news-europe-toi, news-world-cbc, news-politics-cbc, news-africa-bbc, news-asia-bbc, news-europe-bbc, news-latin-america-bbc, news-middle-east-bbc, news-us-bbc, news-world-bbc, news-business-bbc, news-politics-bbc, news-top-dw, news-europe-dw, news-world-dw, news-business-dw, news-asia-dw, zip-demo-ca\n"
     ]
    }
   ],
   "source": [
    "# This cell updates the markdown index files for all the data sources\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "\n",
    "BASE_DIR = Path.cwd() if Path('catalog.csv').exists() else Path.cwd() / 'data'\n",
    "with open(BASE_DIR / 'catalog.csv', newline='') as f:\n",
    "    cat = list(csv.DictReader(f))\n",
    "\n",
    "for row in cat:\n",
    "    folder = BASE_DIR / row['category'] / row['source'] / row['folder']\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    filetype = row['filetype'].strip().lstrip('.')\n",
    "    output_ext = 'json' if filetype.lower() in ('rss', 'xml') else filetype\n",
    "    desc = row['description'].strip()\n",
    "    source = row['source'].strip()\n",
    "    date = row.get('last_fetched', '').strip()\n",
    "\n",
    "    pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}(?:-\\d{2})?\\.\" + re.escape(output_ext) + r\"$\")\n",
    "    dated_files = sorted(p.name for p in folder.iterdir() if pattern.match(p.name))\n",
    "\n",
    "    lines = [\n",
    "        '---',\n",
    "        'layout: default',\n",
    "        f'title: {source} - {desc}',\n",
    "        f'date: {date}',\n",
    "        '---',\n",
    "        '',\n",
    "        f'## {source} - {desc}',\n",
    "        '',\n",
    "        '<div id=\"data-chart\"></div>',\n",
    "        '<div id=\"data-table\"></div>',\n",
    "    ]\n",
    "\n",
    "    if row['source'] == 'fred' and filetype == 'json':\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  ShowChart($('#data-chart'));\",\n",
    "            \"  SourceTabler($('#data-table'));\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "    else:\n",
    "        lines += [\n",
    "            '<script>',\n",
    "            \"document.addEventListener('DOMContentLoaded', function(){\",\n",
    "            \"  document.getElementById('data-table').textContent = 'This source isn\\'t supported for tables yet.';\",\n",
    "            \"});\",\n",
    "            '</script>',\n",
    "        ]\n",
    "\n",
    "    lines += [\n",
    "        '',\n",
    "        '## File Versions:',\n",
    "    ]\n",
    "    links = [f'[Latest version](./latest.{output_ext})'] + [f'[{fname}](./{fname})' for fname in dated_files]\n",
    "    for i, link in enumerate(links, 1):\n",
    "        lines.append(f'{i}. {link}')\n",
    "    (folder / 'index.md').write_text(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "print('Index files generated for', ', '.join(r['folder'] for r in cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25e4e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T05:53:25.413457Z",
     "iopub.status.busy": "2025-07-12T05:53:25.413270Z",
     "iopub.status.idle": "2025-07-12T05:53:32.693789Z",
     "shell.execute_reply": "2025-07-12T05:53:32.693215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb …\n",
      "  Command: /opt/hostedtoolcache/Python/3.13.5/x64/bin/python -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug /home/runner/work/Analysis/Analysis/analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Searching ['/home/runner/.jupyter', '/home/runner/.local/etc/jupyter', '/opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'update_headlines-checkpoint'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['/opt/hostedtoolcache/Python/3.13.5/x64/bin/python', '-m', 'ipykernel_launcher', '-f', '/tmp/tmp7uwxk49u.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:50215\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:34633\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:34633\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:45765\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:45765\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:38351\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:38351\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:60095\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:50215\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:50215\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "print('No external dependencies required.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\nprint('No external dependencies required.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'No external dependencies required.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "# Some of the dependencies, to trigger the dependency checker:\n",
      "# data/news-us/nyt/news-us-nyt/latest.json\n",
      "# data/news-us/wsj/news-us-wsj/latest.json\n",
      "\n",
      "\n",
      "from pathlib import Path\n",
      "import csv\n",
      "import json\n",
      "import xml.etree.ElementTree as ET\n",
      "from datetime import datetime, timezone, timedelta\n",
      "from email.utils import parsedate_to_datetime\n",
      "import shutil\n",
      "\n",
      "BASE_DIR = Path.cwd()\n",
      "REPO_DIR = BASE_DIR\n",
      "while not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):\n",
      "    if REPO_DIR.parent == REPO_DIR:\n",
      "        raise FileNotFoundError('Repository root not found')\n",
      "    REPO_DIR = REPO_DIR.parent\n",
      "DATA_DIR = REPO_DIR / 'data'\n",
      "HEADLINES_DIR = REPO_DIR / 'analysis/headlines'\n",
      "HEADLINES_DIR.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "def parse_pubdate(date_str):\n",
      "    try:\n",
      "        dt = parsedate_to_datetime(date_str) if date_str else None\n",
      "        if dt is None:\n",
      "            return None\n",
      "        if dt.tzinfo is None:\n",
      "            dt = dt.replace(tzinfo=timezone.utc)\n",
      "        return dt.astimezone(timezone.utc)\n",
      "    except Exception:\n",
      "        return None\n",
      "\n",
      "def format_pubdate(dt):\n",
      "    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''\n",
      "\n",
      "def parse_feed(path: Path):\n",
      "    entries = []\n",
      "    if path.suffix == '.json':\n",
      "        with open(path, 'r', encoding='utf-8') as f:\n",
      "            data = json.load(f)\n",
      "        for item in data.get('entries', []):\n",
      "            title = item.get('title')\n",
      "            link = item.get('link')\n",
      "            pub = parse_pubdate(item.get('published'))\n",
      "            if title and link:\n",
      "                entries.append((pub, title.strip(), link.strip()))\n",
      "    else:\n",
      "        try:\n",
      "            tree = ET.parse(path)\n",
      "            root = tree.getroot()\n",
      "        except ET.ParseError:\n",
      "            return entries\n",
      "        for item in root.iter():\n",
      "            if item.tag.lower().endswith(('item', 'entry')):\n",
      "                title = None\n",
      "                link = None\n",
      "                pub = None\n",
      "                for child in item:\n",
      "                    tag = child.tag.lower()\n",
      "                    if tag.endswith('title'):\n",
      "                        title = (child.text or '').strip()\n",
      "                    if tag.endswith('link'):\n",
      "                        link = (child.text or '').strip() or child.attrib.get('href')\n",
      "                    if tag.endswith(('pubdate', 'published', 'updated')):\n",
      "                        pub = parse_pubdate((child.text or '').strip())\n",
      "                if title and link:\n",
      "                    entries.append((pub, title, link))\n",
      "    return entries\n",
      "\n",
      "def collect_headlines():\n",
      "    all_entries = []\n",
      "    for source in DATA_DIR.iterdir():\n",
      "        if source.is_dir() and source.name.startswith('news'):\n",
      "            candidates = [p for p in source.rglob('latest.*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                candidates = [p for p in source.rglob('*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                continue\n",
      "            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\n",
      "            source_name = latest_file.relative_to(DATA_DIR).parts[1]\n",
      "            for pub, title, link in parse_feed(latest_file):\n",
      "                all_entries.append((pub, title, link, source_name))\n",
      "    return all_entries\n",
      "\n",
      "def _date_key(date_str):\n",
      "    try:\n",
      "        return parsedate_to_datetime(date_str) if date_str else datetime.min\n",
      "    except Exception:\n",
      "        return datetime.min\n",
      "\n",
      "def update_headlines():\n",
      "    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\n",
      "    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\n",
      "    if hourly_file.exists():\n",
      "        print(f\"{hourly_file.name} already exists. Skipping update.\")\n",
      "        return\n",
      "    entries = collect_headlines()\n",
      "    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
      "    deduped = []\n",
      "    seen_titles = set()\n",
      "    seen_links = set()\n",
      "    for pub, title, link, src in entries:\n",
      "        t_key = title.lower()\n",
      "        l_key = link.lower()\n",
      "        if t_key in seen_titles or l_key in seen_links:\n",
      "            continue\n",
      "        deduped.append((pub, src, title, link))\n",
      "        seen_titles.add(t_key)\n",
      "        seen_links.add(l_key)\n",
      "    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\n",
      "    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\n",
      "    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        writer.writerow(['pubdate', 'source', 'title', 'link'])\n",
      "        for pub, src, title, link in deduped:\n",
      "            writer.writerow([format_pubdate(pub), src, title, link])\n",
      "    latest_file = HEADLINES_DIR / \"latest.csv\"\n",
      "    shutil.copy(hourly_file, latest_file)\n",
      "    print(f\"Wrote {hourly_file} and updated latest.csv\")\n",
      "    # Get the most recent headline\n",
      "\n",
      "\n",
      "update_headlines()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': '# Some of the dependencies, to trigger the dependency checker:\\n# data/news-us/nyt/news-us-nyt/latest.json\\n# data/news-us/wsj/news-us-wsj/latest.json\\n\\n\\nfrom pathlib import Path\\nimport csv\\nimport json\\nimport xml.etree.ElementTree as ET\\nfrom datetime import datetime, timezone, timedelta\\nfrom email.utils import parsedate_to_datetime\\nimport shutil\\n\\nBASE_DIR = Path.cwd()\\nREPO_DIR = BASE_DIR\\nwhile not ((REPO_DIR / \\'data\\').exists() and (REPO_DIR / \\'analysis\\').exists()):\\n    if REPO_DIR.parent == REPO_DIR:\\n        raise FileNotFoundError(\\'Repository root not found\\')\\n    REPO_DIR = REPO_DIR.parent\\nDATA_DIR = REPO_DIR / \\'data\\'\\nHEADLINES_DIR = REPO_DIR / \\'analysis/headlines\\'\\nHEADLINES_DIR.mkdir(parents=True, exist_ok=True)\\n\\ndef parse_pubdate(date_str):\\n    try:\\n        dt = parsedate_to_datetime(date_str) if date_str else None\\n        if dt is None:\\n            return None\\n        if dt.tzinfo is None:\\n            dt = dt.replace(tzinfo=timezone.utc)\\n        return dt.astimezone(timezone.utc)\\n    except Exception:\\n        return None\\n\\ndef format_pubdate(dt):\\n    return dt.strftime(\\'%Y-%m-%d-%H-%M-%S +0000\\') if dt else \\'\\'\\n\\ndef parse_feed(path: Path):\\n    entries = []\\n    if path.suffix == \\'.json\\':\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            data = json.load(f)\\n        for item in data.get(\\'entries\\', []):\\n            title = item.get(\\'title\\')\\n            link = item.get(\\'link\\')\\n            pub = parse_pubdate(item.get(\\'published\\'))\\n            if title and link:\\n                entries.append((pub, title.strip(), link.strip()))\\n    else:\\n        try:\\n            tree = ET.parse(path)\\n            root = tree.getroot()\\n        except ET.ParseError:\\n            return entries\\n        for item in root.iter():\\n            if item.tag.lower().endswith((\\'item\\', \\'entry\\')):\\n                title = None\\n                link = None\\n                pub = None\\n                for child in item:\\n                    tag = child.tag.lower()\\n                    if tag.endswith(\\'title\\'):\\n                        title = (child.text or \\'\\').strip()\\n                    if tag.endswith(\\'link\\'):\\n                        link = (child.text or \\'\\').strip() or child.attrib.get(\\'href\\')\\n                    if tag.endswith((\\'pubdate\\', \\'published\\', \\'updated\\')):\\n                        pub = parse_pubdate((child.text or \\'\\').strip())\\n                if title and link:\\n                    entries.append((pub, title, link))\\n    return entries\\n\\ndef collect_headlines():\\n    all_entries = []\\n    for source in DATA_DIR.iterdir():\\n        if source.is_dir() and source.name.startswith(\\'news\\'):\\n            candidates = [p for p in source.rglob(\\'latest.*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                candidates = [p for p in source.rglob(\\'*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                continue\\n            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\\n            source_name = latest_file.relative_to(DATA_DIR).parts[1]\\n            for pub, title, link in parse_feed(latest_file):\\n                all_entries.append((pub, title, link, source_name))\\n    return all_entries\\n\\ndef _date_key(date_str):\\n    try:\\n        return parsedate_to_datetime(date_str) if date_str else datetime.min\\n    except Exception:\\n        return datetime.min\\n\\ndef update_headlines():\\n    timestamp = datetime.utcnow().strftime(\\'%Y-%m-%d-%H-00\\')\\n    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\\n    if hourly_file.exists():\\n        print(f\"{hourly_file.name} already exists. Skipping update.\")\\n        return\\n    entries = collect_headlines()\\n    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\\n    deduped = []\\n    seen_titles = set()\\n    seen_links = set()\\n    for pub, title, link, src in entries:\\n        t_key = title.lower()\\n        l_key = link.lower()\\n        if t_key in seen_titles or l_key in seen_links:\\n            continue\\n        deduped.append((pub, src, title, link))\\n        seen_titles.add(t_key)\\n        seen_links.add(l_key)\\n    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\\n    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\\n    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\\n        writer = csv.writer(f)\\n        writer.writerow([\\'pubdate\\', \\'source\\', \\'title\\', \\'link\\'])\\n        for pub, src, title, link in deduped:\\n            writer.writerow([format_pubdate(pub), src, title, link])\\n    latest_file = HEADLINES_DIR / \"latest.csv\"\\n    shutil.copy(hourly_file, latest_file)\\n    print(f\"Wrote {hourly_file} and updated latest.csv\")\\n    # Get the most recent headline\\n\\n\\nupdate_headlines()\\n\\n\\n\\n\\n\\n', 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'Wrote /home/runner/work/Analysis/Analysis/analysis/headlines/2025-07-12-05-00.csv and updated latest.csv\\n'}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"/tmp/ipykernel_2315/2800259878.py:94: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\\n\"}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 3\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7f2f9f0941a0>\n",
      "[NbConvertApp] Writing 8844 bytes to /home/runner/work/Analysis/Analysis/analysis/headlines/.ipynb_checkpoints/update_headlines-checkpoint.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Running analysis/headlines/update_headlines.ipynb …\n",
      "  Command: /opt/hostedtoolcache/Python/3.13.5/x64/bin/python -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Searching ['/home/runner/.jupyter', '/home/runner/.local/etc/jupyter', '/opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'update_headlines'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['/opt/hostedtoolcache/Python/3.13.5/x64/bin/python', '-m', 'ipykernel_launcher', '-f', '/tmp/tmp2j_bcnw2.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:50755\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:59875\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:59875\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:45105\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:45105\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:35009\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:35009\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:37465\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:50755\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:50755\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "print('No external dependencies required.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\nprint('No external dependencies required.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'No external dependencies required.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "# Some of the dependencies, to trigger the dependency checker:\n",
      "# data/news-us/nyt/news-us-nyt/latest.json\n",
      "# data/news-us/wsj/news-us-wsj/latest.json\n",
      "\n",
      "\n",
      "from pathlib import Path\n",
      "import csv\n",
      "import json\n",
      "import xml.etree.ElementTree as ET\n",
      "from datetime import datetime, timezone, timedelta\n",
      "from email.utils import parsedate_to_datetime\n",
      "import shutil\n",
      "\n",
      "BASE_DIR = Path.cwd()\n",
      "REPO_DIR = BASE_DIR\n",
      "while not ((REPO_DIR / 'data').exists() and (REPO_DIR / 'analysis').exists()):\n",
      "    if REPO_DIR.parent == REPO_DIR:\n",
      "        raise FileNotFoundError('Repository root not found')\n",
      "    REPO_DIR = REPO_DIR.parent\n",
      "DATA_DIR = REPO_DIR / 'data'\n",
      "HEADLINES_DIR = REPO_DIR / 'analysis/headlines'\n",
      "HEADLINES_DIR.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "def parse_pubdate(date_str):\n",
      "    try:\n",
      "        dt = parsedate_to_datetime(date_str) if date_str else None\n",
      "        if dt is None:\n",
      "            return None\n",
      "        if dt.tzinfo is None:\n",
      "            dt = dt.replace(tzinfo=timezone.utc)\n",
      "        return dt.astimezone(timezone.utc)\n",
      "    except Exception:\n",
      "        return None\n",
      "\n",
      "def format_pubdate(dt):\n",
      "    return dt.strftime('%Y-%m-%d-%H-%M-%S +0000') if dt else ''\n",
      "\n",
      "def parse_feed(path: Path):\n",
      "    entries = []\n",
      "    if path.suffix == '.json':\n",
      "        with open(path, 'r', encoding='utf-8') as f:\n",
      "            data = json.load(f)\n",
      "        for item in data.get('entries', []):\n",
      "            title = item.get('title')\n",
      "            link = item.get('link')\n",
      "            pub = parse_pubdate(item.get('published'))\n",
      "            if title and link:\n",
      "                entries.append((pub, title.strip(), link.strip()))\n",
      "    else:\n",
      "        try:\n",
      "            tree = ET.parse(path)\n",
      "            root = tree.getroot()\n",
      "        except ET.ParseError:\n",
      "            return entries\n",
      "        for item in root.iter():\n",
      "            if item.tag.lower().endswith(('item', 'entry')):\n",
      "                title = None\n",
      "                link = None\n",
      "                pub = None\n",
      "                for child in item:\n",
      "                    tag = child.tag.lower()\n",
      "                    if tag.endswith('title'):\n",
      "                        title = (child.text or '').strip()\n",
      "                    if tag.endswith('link'):\n",
      "                        link = (child.text or '').strip() or child.attrib.get('href')\n",
      "                    if tag.endswith(('pubdate', 'published', 'updated')):\n",
      "                        pub = parse_pubdate((child.text or '').strip())\n",
      "                if title and link:\n",
      "                    entries.append((pub, title, link))\n",
      "    return entries\n",
      "\n",
      "def collect_headlines():\n",
      "    all_entries = []\n",
      "    feed_info = {}\n",
      "    for source in DATA_DIR.iterdir():\n",
      "        if source.is_dir() and source.name.startswith('news'):\n",
      "            candidates = [p for p in source.rglob('latest.*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                candidates = [p for p in source.rglob('*') if p.suffix in {'.json', '.rss', '.xml'}]\n",
      "            if not candidates:\n",
      "                continue\n",
      "            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\n",
      "            source_name = latest_file.relative_to(DATA_DIR).parts[1]\n",
      "            feed_entries = parse_feed(latest_file)\n",
      "            if feed_entries:\n",
      "                recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)\n",
      "                feed_info[source_name] = {'count': len(feed_entries), 'recent': recent}\n",
      "            for pub, title, link in feed_entries:\n",
      "                all_entries.append((pub, title, link, source_name))\n",
      "    return all_entries, feed_info\n",
      "\n",
      "def _date_key(date_str):\n",
      "    try:\n",
      "        return parsedate_to_datetime(date_str) if date_str else datetime.min\n",
      "    except Exception:\n",
      "        return datetime.min\n",
      "\n",
      "def update_headlines():\n",
      "    timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\n",
      "    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\n",
      "    if hourly_file.exists():\n",
      "        print(f\"{hourly_file.name} already exists. Skipping update.\")\n",
      "        return\n",
      "    entries, feed_info = collect_headlines()\n",
      "    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
      "    deduped = []\n",
      "    seen_titles = set()\n",
      "    seen_links = set()\n",
      "    for pub, title, link, src in entries:\n",
      "        t_key = title.lower()\n",
      "        l_key = link.lower()\n",
      "        if t_key in seen_titles or l_key in seen_links:\n",
      "            continue\n",
      "        deduped.append((pub, src, title, link))\n",
      "        seen_titles.add(t_key)\n",
      "        seen_links.add(l_key)\n",
      "    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\n",
      "    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\n",
      "    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        writer.writerow(['pubdate', 'source', 'title', 'link'])\n",
      "        for pub, src, title, link in deduped:\n",
      "            writer.writerow([format_pubdate(pub), src, title, link])\n",
      "    latest_file = HEADLINES_DIR / \"latest.csv\"\n",
      "    shutil.copy(hourly_file, latest_file)\n",
      "    print(f\"Wrote {hourly_file} and updated latest.csv\")\n",
      "    print()\n",
      "    print('Feed summary:')\n",
      "    print(f\"{'source':<20} {'count':>5}  {'most recent'}\")\n",
      "    for src, info in sorted(feed_info.items()):\n",
      "        print(f\"{src:<20} {info['count']:5}  {format_pubdate(info['recent'])}\")\n",
      "    # Get the most recent headline\n",
      "\n",
      "\n",
      "update_headlines()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': '# Some of the dependencies, to trigger the dependency checker:\\n# data/news-us/nyt/news-us-nyt/latest.json\\n# data/news-us/wsj/news-us-wsj/latest.json\\n\\n\\nfrom pathlib import Path\\nimport csv\\nimport json\\nimport xml.etree.ElementTree as ET\\nfrom datetime import datetime, timezone, timedelta\\nfrom email.utils import parsedate_to_datetime\\nimport shutil\\n\\nBASE_DIR = Path.cwd()\\nREPO_DIR = BASE_DIR\\nwhile not ((REPO_DIR / \\'data\\').exists() and (REPO_DIR / \\'analysis\\').exists()):\\n    if REPO_DIR.parent == REPO_DIR:\\n        raise FileNotFoundError(\\'Repository root not found\\')\\n    REPO_DIR = REPO_DIR.parent\\nDATA_DIR = REPO_DIR / \\'data\\'\\nHEADLINES_DIR = REPO_DIR / \\'analysis/headlines\\'\\nHEADLINES_DIR.mkdir(parents=True, exist_ok=True)\\n\\ndef parse_pubdate(date_str):\\n    try:\\n        dt = parsedate_to_datetime(date_str) if date_str else None\\n        if dt is None:\\n            return None\\n        if dt.tzinfo is None:\\n            dt = dt.replace(tzinfo=timezone.utc)\\n        return dt.astimezone(timezone.utc)\\n    except Exception:\\n        return None\\n\\ndef format_pubdate(dt):\\n    return dt.strftime(\\'%Y-%m-%d-%H-%M-%S +0000\\') if dt else \\'\\'\\n\\ndef parse_feed(path: Path):\\n    entries = []\\n    if path.suffix == \\'.json\\':\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            data = json.load(f)\\n        for item in data.get(\\'entries\\', []):\\n            title = item.get(\\'title\\')\\n            link = item.get(\\'link\\')\\n            pub = parse_pubdate(item.get(\\'published\\'))\\n            if title and link:\\n                entries.append((pub, title.strip(), link.strip()))\\n    else:\\n        try:\\n            tree = ET.parse(path)\\n            root = tree.getroot()\\n        except ET.ParseError:\\n            return entries\\n        for item in root.iter():\\n            if item.tag.lower().endswith((\\'item\\', \\'entry\\')):\\n                title = None\\n                link = None\\n                pub = None\\n                for child in item:\\n                    tag = child.tag.lower()\\n                    if tag.endswith(\\'title\\'):\\n                        title = (child.text or \\'\\').strip()\\n                    if tag.endswith(\\'link\\'):\\n                        link = (child.text or \\'\\').strip() or child.attrib.get(\\'href\\')\\n                    if tag.endswith((\\'pubdate\\', \\'published\\', \\'updated\\')):\\n                        pub = parse_pubdate((child.text or \\'\\').strip())\\n                if title and link:\\n                    entries.append((pub, title, link))\\n    return entries\\n\\ndef collect_headlines():\\n    all_entries = []\\n    feed_info = {}\\n    for source in DATA_DIR.iterdir():\\n        if source.is_dir() and source.name.startswith(\\'news\\'):\\n            candidates = [p for p in source.rglob(\\'latest.*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                candidates = [p for p in source.rglob(\\'*\\') if p.suffix in {\\'.json\\', \\'.rss\\', \\'.xml\\'}]\\n            if not candidates:\\n                continue\\n            latest_file = max(candidates, key=lambda p: p.stat().st_mtime)\\n            source_name = latest_file.relative_to(DATA_DIR).parts[1]\\n            feed_entries = parse_feed(latest_file)\\n            if feed_entries:\\n                recent = max((pub for pub, _t, _l in feed_entries if pub), default=None)\\n                feed_info[source_name] = {\\'count\\': len(feed_entries), \\'recent\\': recent}\\n            for pub, title, link in feed_entries:\\n                all_entries.append((pub, title, link, source_name))\\n    return all_entries, feed_info\\n\\ndef _date_key(date_str):\\n    try:\\n        return parsedate_to_datetime(date_str) if date_str else datetime.min\\n    except Exception:\\n        return datetime.min\\n\\ndef update_headlines():\\n    timestamp = datetime.utcnow().strftime(\\'%Y-%m-%d-%H-00\\')\\n    hourly_file = HEADLINES_DIR / f\"{timestamp}.csv\"\\n    if hourly_file.exists():\\n        print(f\"{hourly_file.name} already exists. Skipping update.\")\\n        return\\n    entries, feed_info = collect_headlines()\\n    entries.sort(key=lambda r: r[0] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\\n    deduped = []\\n    seen_titles = set()\\n    seen_links = set()\\n    for pub, title, link, src in entries:\\n        t_key = title.lower()\\n        l_key = link.lower()\\n        if t_key in seen_titles or l_key in seen_links:\\n            continue\\n        deduped.append((pub, src, title, link))\\n        seen_titles.add(t_key)\\n        seen_links.add(l_key)\\n    cutoff = datetime.now(timezone.utc) - timedelta(days=1)\\n    deduped = [r for r in deduped if r[0] and r[0] >= cutoff]\\n    with open(hourly_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\\n        writer = csv.writer(f)\\n        writer.writerow([\\'pubdate\\', \\'source\\', \\'title\\', \\'link\\'])\\n        for pub, src, title, link in deduped:\\n            writer.writerow([format_pubdate(pub), src, title, link])\\n    latest_file = HEADLINES_DIR / \"latest.csv\"\\n    shutil.copy(hourly_file, latest_file)\\n    print(f\"Wrote {hourly_file} and updated latest.csv\")\\n    print()\\n    print(\\'Feed summary:\\')\\n    print(f\"{\\'source\\':<20} {\\'count\\':>5}  {\\'most recent\\'}\")\\n    for src, info in sorted(feed_info.items()):\\n        print(f\"{src:<20} {info[\\'count\\']:5}  {format_pubdate(info[\\'recent\\'])}\")\\n    # Get the most recent headline\\n\\n\\nupdate_headlines()\\n\\n\\n\\n\\n\\n', 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': '2025-07-12-05-00.csv already exists. Skipping update.\\n'}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"/tmp/ipykernel_2333/1894882967.py:99: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00')\\n\"}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7f6fe0700050>\n",
      "[NbConvertApp] Writing 9209 bytes to /home/runner/work/Analysis/Analysis/analysis/headlines/update_headlines.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Running analysis/news-topics/analyze_headlines.ipynb …\n",
      "  Command: /opt/hostedtoolcache/Python/3.13.5/x64/bin/python -m jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=600 --debug /home/runner/work/Analysis/Analysis/analysis/news-topics/analyze_headlines.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Searching ['/home/runner/.jupyter', '/home/runner/.local/etc/jupyter', '/opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files\n",
      "[NbConvertApp] Looking for jupyter_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /opt/hostedtoolcache/Python/3.13.5/x64/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.local/etc/jupyter\n",
      "[NbConvertApp] Looking for jupyter_nbconvert_config in /home/runner/.jupyter\n",
      "[NbConvertApp] Looping through config variables with prefix \"JUPYTER_NBCONVERT\"\n",
      "[NbConvertApp] Converting notebook /home/runner/work/Analysis/Analysis/analysis/news-topics/analyze_headlines.ipynb to notebook\n",
      "[NbConvertApp] Notebook name is 'analyze_headlines'\n",
      "[NbConvertApp] Applying preprocessor: ExecutePreprocessor\n",
      "[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner\n",
      "[NbConvertApp] Starting kernel: ['/opt/hostedtoolcache/Python/3.13.5/x64/bin/python', '-m', 'ipykernel_launcher', '-f', '/tmp/tmp181y39qg.json', '--HistoryManager.hist_file=:memory:']\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:48265\n",
      "[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:39845\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:39845\n",
      "[NbConvertApp] connecting shell channel to tcp://127.0.0.1:39883\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:39883\n",
      "[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:44899\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:44899\n",
      "[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:38101\n",
      "[NbConvertApp] connecting control channel to tcp://127.0.0.1:48265\n",
      "[NbConvertApp] Connecting to: tcp://127.0.0.1:48265\n",
      "[NbConvertApp] Skipping non-executing cell 0\n",
      "[NbConvertApp] Executing cell:\n",
      "# ========== Bootstrap: ensure required Python packages are present =========\n",
      "import importlib, subprocess, sys\n",
      "from typing import Optional\n",
      "\n",
      "def _ensure(pkg_name: str, import_name: Optional[str] = None):\n",
      "    try:\n",
      "        importlib.import_module(import_name or pkg_name)\n",
      "    except ModuleNotFoundError:\n",
      "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
      "    finally:\n",
      "        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\n",
      "\n",
      "_ensure('pandas')\n",
      "print('All dependencies ready.\\n')\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"# ========== Bootstrap: ensure required Python packages are present =========\\nimport importlib, subprocess, sys\\nfrom typing import Optional\\n\\ndef _ensure(pkg_name: str, import_name: Optional[str] = None):\\n    try:\\n        importlib.import_module(import_name or pkg_name)\\n    except ModuleNotFoundError:\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\\n    finally:\\n        globals()[import_name or pkg_name] = importlib.import_module(import_name or pkg_name)\\n\\n_ensure('pandas')\\nprint('All dependencies ready.\\\\n')\\n\", 'execution_count': 1}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stdout', 'text': 'All dependencies ready.\\n\\n'}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Executing cell:\n",
      "import pandas as pd\n",
      "latest = pd.read_csv('../headlines/latest.csv')\n",
      "latest.head()\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import pandas as pd\\nlatest = pd.read_csv('../headlines/latest.csv')\\nlatest.head()\", 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': \"                     pubdate   source  \\\\\\n0  2025-07-12-04-20-41 +0000      bbc   \\n1  2025-07-12-04-01-07 +0000      nyt   \\n2  2025-07-12-03-00-16 +0000      bbc   \\n3  2025-07-12-02-59-22 +0000      bbc   \\n4  2025-07-12-01-58-41 +0000  latimes   \\n\\n                                               title  \\\\\\n0  Gaza ceasefire talks on verge of collapse, Pal...   \\n1  How to Panic Italians? Jack Up the Price of Es...   \\n2  Federal judge says voice-over artists' AI laws...   \\n3  Why cockpit audio deepens the mystery of Air I...   \\n4  Man sets California probation officer on fire ...   \\n\\n                                                link  \\n0     https://www.bbc.com/news/articles/cqjq9p87vdvo  \\n1  https://www.nytimes.com/2025/07/12/world/europ...  \\n2     https://www.bbc.com/news/articles/cedgzj8z1wjo  \\n3     https://www.bbc.com/news/articles/cx2gy78gpnqo  \\n4  https://www.latimes.com/california/story/2025-...  \", 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>2025-07-12-04-20-41 +0000</td>\\n      <td>bbc</td>\\n      <td>Gaza ceasefire talks on verge of collapse, Pal...</td>\\n      <td>https://www.bbc.com/news/articles/cqjq9p87vdvo</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2025-07-12-04-01-07 +0000</td>\\n      <td>nyt</td>\\n      <td>How to Panic Italians? Jack Up the Price of Es...</td>\\n      <td>https://www.nytimes.com/2025/07/12/world/europ...</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2025-07-12-03-00-16 +0000</td>\\n      <td>bbc</td>\\n      <td>Federal judge says voice-over artists\\' AI laws...</td>\\n      <td>https://www.bbc.com/news/articles/cedgzj8z1wjo</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>2025-07-12-02-59-22 +0000</td>\\n      <td>bbc</td>\\n      <td>Why cockpit audio deepens the mystery of Air I...</td>\\n      <td>https://www.bbc.com/news/articles/cx2gy78gpnqo</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>2025-07-12-01-58-41 +0000</td>\\n      <td>latimes</td>\\n      <td>Man sets California probation officer on fire ...</td>\\n      <td>https://www.latimes.com/california/story/2025-...</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 2}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 3\n",
      "[NbConvertApp] Executing cell:\n",
      "import re\n",
      "from collections import Counter\n",
      "from datetime import datetime\n",
      "\n",
      "with open('exclude.txt') as f:\n",
      "    stop_words = set(w.strip() for w in f if w.strip())\n",
      "words = re.findall(r'[A-Za-z]+', ' '.join(latest['title']).lower())\n",
      "filtered = [w for w in words if w not in stop_words and len(w) > 1]\n",
      "counts = Counter(filtered)\n",
      "score_df = (\n",
      "    pd.DataFrame(counts.items(), columns=['word','score'])\n",
      "    .sort_values('score', ascending=False)\n",
      ")\n",
      "score_df[['score','word']].to_csv('scores.csv', index=False)\n",
      "timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\n",
      "score_df[['score','word']].to_csv(f'scores-{timestamp}.csv', index=False)\n",
      "score_df.head()\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import re\\nfrom collections import Counter\\nfrom datetime import datetime\\n\\nwith open('exclude.txt') as f:\\n    stop_words = set(w.strip() for w in f if w.strip())\\nwords = re.findall(r'[A-Za-z]+', ' '.join(latest['title']).lower())\\nfiltered = [w for w in words if w not in stop_words and len(w) > 1]\\ncounts = Counter(filtered)\\nscore_df = (\\n    pd.DataFrame(counts.items(), columns=['word','score'])\\n    .sort_values('score', ascending=False)\\n)\\nscore_df[['score','word']].to_csv('scores.csv', index=False)\\ntimestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\\nscore_df[['score','word']].to_csv(f'scores-{timestamp}.csv', index=False)\\nscore_df.head()\\n\", 'execution_count': 3}\n",
      "[NbConvertApp] msg_type: stream\n",
      "[NbConvertApp] content: {'name': 'stderr', 'text': \"/tmp/ipykernel_2351/2741963311.py:15: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\\n  timestamp = datetime.utcnow().strftime('%Y-%m-%d-%H-00-00')\\n\"}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': '            word  score\\n127        trump     13\\n51   immigration      6\\n153       canada      5\\n297         will      5\\n27    california      4', 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>word</th>\\n      <th>score</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>127</th>\\n      <td>trump</td>\\n      <td>13</td>\\n    </tr>\\n    <tr>\\n      <th>51</th>\\n      <td>immigration</td>\\n      <td>6</td>\\n    </tr>\\n    <tr>\\n      <th>153</th>\\n      <td>canada</td>\\n      <td>5</td>\\n    </tr>\\n    <tr>\\n      <th>297</th>\\n      <td>will</td>\\n      <td>5</td>\\n    </tr>\\n    <tr>\\n      <th>27</th>\\n      <td>california</td>\\n      <td>4</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 3}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 5\n",
      "[NbConvertApp] Executing cell:\n",
      "word_scores = dict(score_df[['word','score']].values)\n",
      "latest['score'] = latest['title'].apply(\n",
      "    lambda t: sum(\n",
      "        word_scores.get(w.lower(), 0)\n",
      "        for w in re.findall(r'[A-Za-z]+', t)\n",
      "        if len(w) > 1\n",
      "    )\n",
      ")\n",
      "ranked = latest.sort_values('score', ascending=False)\n",
      "ranked[['score','pubdate','source','title','link']].to_csv('rank.csv', index=False)\n",
      "ranked[['score','pubdate','source','title','link']].to_csv(f'rank-{timestamp}.csv', index=False)\n",
      "ranked.head()\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"word_scores = dict(score_df[['word','score']].values)\\nlatest['score'] = latest['title'].apply(\\n    lambda t: sum(\\n        word_scores.get(w.lower(), 0)\\n        for w in re.findall(r'[A-Za-z]+', t)\\n        if len(w) > 1\\n    )\\n)\\nranked = latest.sort_values('score', ascending=False)\\nranked[['score','pubdate','source','title','link']].to_csv('rank.csv', index=False)\\nranked[['score','pubdate','source','title','link']].to_csv(f'rank-{timestamp}.csv', index=False)\\nranked.head()\\n\", 'execution_count': 4}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': \"                      pubdate source  \\\\\\n60  2025-07-11-14-53-00 +0000    wsj   \\n37  2025-07-11-19-40-14 +0000    nyt   \\n29  2025-07-11-21-50-58 +0000    nyt   \\n40  2025-07-11-19-04-37 +0000    nyt   \\n20  2025-07-11-23-05-58 +0000    bbc   \\n\\n                                                title  \\\\\\n60  Businesses hoping for clarity on where Trump t...   \\n37  Trump’s Latest Canada Tariff Threats Come Desp...   \\n29  As Trump Sows Tariff Confusion, Rules of Globa...   \\n40  Trump Says NATO Countries Will Buy Weapons to ...   \\n20  Canada's Carney talked tough on Trump - now so...   \\n\\n                                                 link  score  \\n60  https://www.wsj.com/economy/trade/trump-tariff...     35  \\n37  https://www.nytimes.com/2025/07/11/world/canad...     31  \\n29  https://www.nytimes.com/2025/07/11/world/europ...     29  \\n40  https://www.nytimes.com/2025/07/11/world/europ...     29  \\n20     https://www.bbc.com/news/articles/cvg81518yyyo     25  \", 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n      <th>score</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>60</th>\\n      <td>2025-07-11-14-53-00 +0000</td>\\n      <td>wsj</td>\\n      <td>Businesses hoping for clarity on where Trump t...</td>\\n      <td>https://www.wsj.com/economy/trade/trump-tariff...</td>\\n      <td>35</td>\\n    </tr>\\n    <tr>\\n      <th>37</th>\\n      <td>2025-07-11-19-40-14 +0000</td>\\n      <td>nyt</td>\\n      <td>Trump’s Latest Canada Tariff Threats Come Desp...</td>\\n      <td>https://www.nytimes.com/2025/07/11/world/canad...</td>\\n      <td>31</td>\\n    </tr>\\n    <tr>\\n      <th>29</th>\\n      <td>2025-07-11-21-50-58 +0000</td>\\n      <td>nyt</td>\\n      <td>As Trump Sows Tariff Confusion, Rules of Globa...</td>\\n      <td>https://www.nytimes.com/2025/07/11/world/europ...</td>\\n      <td>29</td>\\n    </tr>\\n    <tr>\\n      <th>40</th>\\n      <td>2025-07-11-19-04-37 +0000</td>\\n      <td>nyt</td>\\n      <td>Trump Says NATO Countries Will Buy Weapons to ...</td>\\n      <td>https://www.nytimes.com/2025/07/11/world/europ...</td>\\n      <td>29</td>\\n    </tr>\\n    <tr>\\n      <th>20</th>\\n      <td>2025-07-11-23-05-58 +0000</td>\\n      <td>bbc</td>\\n      <td>Canada\\'s Carney talked tough on Trump - now so...</td>\\n      <td>https://www.bbc.com/news/articles/cvg81518yyyo</td>\\n      <td>25</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 4}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 7\n",
      "[NbConvertApp] Executing cell:\n",
      "top_rows = []\n",
      "working = word_scores.copy()\n",
      "remaining = latest.copy()\n",
      "for _ in range(10):\n",
      "    ranked_loop = remaining.assign(score=remaining['title'].apply(\n",
      "        lambda t: sum(working.get(w.lower(), 0)\n",
      "                      for w in re.findall(r'[A-Za-z]+', t)\n",
      "                      if len(w) > 1)\n",
      "    )).sort_values('score', ascending=False)\n",
      "    if ranked_loop.empty:\n",
      "        break\n",
      "    top_story = ranked_loop.iloc[0]\n",
      "    top_rows.append(top_story[['score','pubdate','source','title','link']])\n",
      "    words = set(re.findall(r'[A-Za-z]+', top_story['title'].lower()))\n",
      "    for w in words:\n",
      "        working.pop(w, None)\n",
      "    remaining = remaining.drop(top_story.name)\n",
      "top_df = pd.DataFrame(top_rows)\n",
      "top_df.to_csv('top.csv', index=False)\n",
      "top_df.to_csv(f'top-{timestamp}.csv', index=False)\n",
      "top_df\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"top_rows = []\\nworking = word_scores.copy()\\nremaining = latest.copy()\\nfor _ in range(10):\\n    ranked_loop = remaining.assign(score=remaining['title'].apply(\\n        lambda t: sum(working.get(w.lower(), 0)\\n                      for w in re.findall(r'[A-Za-z]+', t)\\n                      if len(w) > 1)\\n    )).sort_values('score', ascending=False)\\n    if ranked_loop.empty:\\n        break\\n    top_story = ranked_loop.iloc[0]\\n    top_rows.append(top_story[['score','pubdate','source','title','link']])\\n    words = set(re.findall(r'[A-Za-z]+', top_story['title'].lower()))\\n    for w in words:\\n        working.pop(w, None)\\n    remaining = remaining.drop(top_story.name)\\ntop_df = pd.DataFrame(top_rows)\\ntop_df.to_csv('top.csv', index=False)\\ntop_df.to_csv(f'top-{timestamp}.csv', index=False)\\ntop_df\\n\", 'execution_count': 5}\n",
      "[NbConvertApp] msg_type: execute_result\n",
      "[NbConvertApp] content: {'data': {'text/plain': \"    score                    pubdate   source  \\\\\\n60     35  2025-07-11-14-53-00 +0000      wsj   \\n9      23  2025-07-12-01-09-58 +0000  latimes   \\n5      20  2025-07-12-01-53-00 +0000      wsj   \\n50     19  2025-07-11-16-57-34 +0000      bbc   \\n81     19  2025-07-11-10-00-00 +0000  latimes   \\n37     18  2025-07-11-19-40-14 +0000      nyt   \\n47     17  2025-07-11-17-37-00 +0000      wsj   \\n69     15  2025-07-11-13-09-38 +0000      nyt   \\n16     14  2025-07-12-00-16-13 +0000  latimes   \\n78     14  2025-07-11-10-00-00 +0000  latimes   \\n\\n                                                title  \\\\\\n60  Businesses hoping for clarity on where Trump t...   \\n9   California mayor warns residents amid immigrat...   \\n5   A dispute over the Federal Reserve’s renovatio...   \\n50  Mother mourns 'beautiful' 12-year-old shot whi...   \\n81  Arellano: 'La migra, la migra': Inside Hunting...   \\n37  Trump’s Latest Canada Tariff Threats Come Desp...   \\n47  Nuclear power exemplifies how revamping dated ...   \\n69  Kurdish PKK Fighters Burn Weapons in Step Towa...   \\n16  SoCal surgery center staff confront ICE agents...   \\n78  How a Supreme Court win for public health bols...   \\n\\n                                                 link  \\n60  https://www.wsj.com/economy/trade/trump-tariff...  \\n9   https://www.latimes.com/california/story/2025-...  \\n5   https://www.wsj.com/economy/central-banking/je...  \\n50     https://www.bbc.com/news/articles/cd6gzx85gdqo  \\n81  https://www.latimes.com/california/story/2025-...  \\n37  https://www.nytimes.com/2025/07/11/world/canad...  \\n47  https://www.wsj.com/economy/trumps-unsung-econ...  \\n69  https://www.nytimes.com/2025/07/11/world/middl...  \\n16  https://www.latimes.com/california/story/2025-...  \\n78  https://www.latimes.com/politics/story/2025-07...  \", 'text/html': '<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>score</th>\\n      <th>pubdate</th>\\n      <th>source</th>\\n      <th>title</th>\\n      <th>link</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>60</th>\\n      <td>35</td>\\n      <td>2025-07-11-14-53-00 +0000</td>\\n      <td>wsj</td>\\n      <td>Businesses hoping for clarity on where Trump t...</td>\\n      <td>https://www.wsj.com/economy/trade/trump-tariff...</td>\\n    </tr>\\n    <tr>\\n      <th>9</th>\\n      <td>23</td>\\n      <td>2025-07-12-01-09-58 +0000</td>\\n      <td>latimes</td>\\n      <td>California mayor warns residents amid immigrat...</td>\\n      <td>https://www.latimes.com/california/story/2025-...</td>\\n    </tr>\\n    <tr>\\n      <th>5</th>\\n      <td>20</td>\\n      <td>2025-07-12-01-53-00 +0000</td>\\n      <td>wsj</td>\\n      <td>A dispute over the Federal Reserve’s renovatio...</td>\\n      <td>https://www.wsj.com/economy/central-banking/je...</td>\\n    </tr>\\n    <tr>\\n      <th>50</th>\\n      <td>19</td>\\n      <td>2025-07-11-16-57-34 +0000</td>\\n      <td>bbc</td>\\n      <td>Mother mourns \\'beautiful\\' 12-year-old shot whi...</td>\\n      <td>https://www.bbc.com/news/articles/cd6gzx85gdqo</td>\\n    </tr>\\n    <tr>\\n      <th>81</th>\\n      <td>19</td>\\n      <td>2025-07-11-10-00-00 +0000</td>\\n      <td>latimes</td>\\n      <td>Arellano: \\'La migra, la migra\\': Inside Hunting...</td>\\n      <td>https://www.latimes.com/california/story/2025-...</td>\\n    </tr>\\n    <tr>\\n      <th>37</th>\\n      <td>18</td>\\n      <td>2025-07-11-19-40-14 +0000</td>\\n      <td>nyt</td>\\n      <td>Trump’s Latest Canada Tariff Threats Come Desp...</td>\\n      <td>https://www.nytimes.com/2025/07/11/world/canad...</td>\\n    </tr>\\n    <tr>\\n      <th>47</th>\\n      <td>17</td>\\n      <td>2025-07-11-17-37-00 +0000</td>\\n      <td>wsj</td>\\n      <td>Nuclear power exemplifies how revamping dated ...</td>\\n      <td>https://www.wsj.com/economy/trumps-unsung-econ...</td>\\n    </tr>\\n    <tr>\\n      <th>69</th>\\n      <td>15</td>\\n      <td>2025-07-11-13-09-38 +0000</td>\\n      <td>nyt</td>\\n      <td>Kurdish PKK Fighters Burn Weapons in Step Towa...</td>\\n      <td>https://www.nytimes.com/2025/07/11/world/middl...</td>\\n    </tr>\\n    <tr>\\n      <th>16</th>\\n      <td>14</td>\\n      <td>2025-07-12-00-16-13 +0000</td>\\n      <td>latimes</td>\\n      <td>SoCal surgery center staff confront ICE agents...</td>\\n      <td>https://www.latimes.com/california/story/2025-...</td>\\n    </tr>\\n    <tr>\\n      <th>78</th>\\n      <td>14</td>\\n      <td>2025-07-11-10-00-00 +0000</td>\\n      <td>latimes</td>\\n      <td>How a Supreme Court win for public health bols...</td>\\n      <td>https://www.latimes.com/politics/story/2025-07...</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>'}, 'metadata': {}, 'execution_count': 5}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 9\n",
      "[NbConvertApp] Executing cell:\n",
      "import pandas as pd\n",
      "pd.read_csv('top.csv').to_json('top.json', orient='records', indent=2)\n",
      "\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'busy'}\n",
      "[NbConvertApp] msg_type: execute_input\n",
      "[NbConvertApp] content: {'code': \"import pandas as pd\\npd.read_csv('top.csv').to_json('top.json', orient='records', indent=2)\\n\", 'execution_count': 6}\n",
      "[NbConvertApp] msg_type: status\n",
      "[NbConvertApp] content: {'execution_state': 'idle'}\n",
      "[NbConvertApp] Skipping non-executing cell 11\n",
      "[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7f5073aa4050>\n",
      "[NbConvertApp] Writing 23646 bytes to /home/runner/work/Analysis/Analysis/analysis/news-topics/analyze_headlines.ipynb\n",
      "\n",
      "  ✓ success\n",
      "Everything up to date.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update outdated notebooks until none remain\n",
    "import json, re, time, subprocess, sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_dir = Path.cwd()\n",
    "if not (repo_dir / 'analysis').is_dir():\n",
    "    repo_dir = repo_dir.parent\n",
    "analysis_dir = repo_dir / 'analysis'\n",
    "data_dir = repo_dir / 'data'\n",
    "\n",
    "pattern = re.compile(r'[A-Za-z0-9_/.-]*latest\\.(?:csv|json|xml|rss)')\n",
    "\n",
    "def mtime_str(p: Path) -> str:\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(p.stat().st_mtime))\n",
    "\n",
    "def build_dep_map():\n",
    "    ipynb_paths = sorted(analysis_dir.rglob('*.ipynb'))\n",
    "    dep_map = {}\n",
    "    for nb in ipynb_paths:\n",
    "        text = nb.read_text()\n",
    "        matches = sorted(set(pattern.findall(text)))\n",
    "        deps = []\n",
    "        for m in matches:\n",
    "            dep = (nb.parent / m).resolve()\n",
    "            if not dep.exists():\n",
    "                dep = (repo_dir / m.lstrip('./')).resolve()\n",
    "            if dep.exists():\n",
    "                deps.append(dep)\n",
    "        dep_map[nb] = deps\n",
    "    return dep_map\n",
    "\n",
    "def outdated(nb, deps):\n",
    "    nb_mtime = nb.stat().st_mtime\n",
    "    return any(d.stat().st_mtime > nb_mtime for d in deps)\n",
    "\n",
    "\n",
    "def execute(nb: Path):\n",
    "    import shutil\n",
    "    rel = str(nb.relative_to(repo_dir))\n",
    "    if not shutil.which('jupyter'):\n",
    "        print(f'jupyter not available - skipping {rel}')\n",
    "        return\n",
    "    print(f'Running {rel} …')\n",
    "    cmd=[sys.executable,'-m','jupyter','nbconvert','--to','notebook','--inplace','--execute','--ExecutePreprocessor.timeout=600','--debug',str(nb)]\n",
    "    print('  Command:', ' '.join(cmd))\n",
    "    proc=subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    print(proc.stdout)\n",
    "    if proc.returncode==0:\n",
    "        print('  ✓ success')\n",
    "    else:\n",
    "        print(f'  ✗ failed with exit code {proc.returncode}')\n",
    "while True:\n",
    "    dep_map=build_dep_map()\n",
    "    outdated_nbs=[nb for nb,deps in dep_map.items() if deps and outdated(nb,deps)]\n",
    "    report={'outdated_notebooks':[str(nb.relative_to(repo_dir)) for nb in outdated_nbs]}\n",
    "    deps_path = repo_dir / 'dependencies.json'\n",
    "    deps_path.write_text(\n",
    "        json.dumps(report, indent=2) + \"\\n\"\n",
    "    )\n",
    "    if not outdated_nbs:\n",
    "        print('Everything up to date.')\n",
    "        break\n",
    "    for nb in outdated_nbs:\n",
    "        execute(nb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
